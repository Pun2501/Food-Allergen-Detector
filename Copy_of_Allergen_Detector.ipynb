{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fb-EmoZJIwM5"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers torch Pillow matplotlib pytesseract\n",
        "!sudo apt install tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-YrxL3HI0qN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dO1X8Zx-KEtJ"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")\n",
        "print(ds)  # Check dataset structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSVGJ-8ILqvS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")\n",
        "\n",
        "# Check the structure of the train split\n",
        "print(ds['train'].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf5KdwwGL1Yp"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW1f4chDLy9B"
      },
      "outputs": [],
      "source": [
        "# Inspect the first sample\n",
        "print(ds[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PtbwEH6NNG7"
      },
      "source": [
        "Visualizing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbRT6zi0L7wA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Visualize a few random images from the dataset\n",
        "for i in range(3):  # Displaying 3 random images\n",
        "    image_path = ds[\"train\"][i][\"image\"]\n",
        "    img = image_path # Directly assign image_path to img, it's already a PIL Image object\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Text: {ds['train'][i]['text']}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ8brnRpME8k"
      },
      "outputs": [],
      "source": [
        "# View sample entry\n",
        "sample = ds[\"train\"][0]\n",
        "print(\"Keys in sample:\", sample.keys())\n",
        "\n",
        "# Check if images need OCR processing\n",
        "if \"image\" in sample:\n",
        "    display(sample[\"image\"])  # Show image\n",
        "    print(pytesseract.image_to_string(sample[\"image\"]))  # OCR test\n",
        "\n",
        "# Check annotations\n",
        "print(\"Annotations:\", sample.get(\"annotations\", \"No annotations\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9u-uQ2ONJXt"
      },
      "source": [
        "Check the Distribution of Ingredients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGTTZr8vMRBA"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten ner_tags and count the occurrence of each tag\n",
        "ner_tags = [tag for sample in ds[\"train\"] for tag in sample[\"ner_tags\"]]\n",
        "tag_counts = Counter(ner_tags)\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(tag_counts.keys(), tag_counts.values())\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"NER Tags\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"NER Tag Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h64Cwnr4ODzT"
      },
      "source": [
        "Text Length Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMK0MpUVM9WG"
      },
      "outputs": [],
      "source": [
        "# Calculate text lengths\n",
        "text_lengths = [len(sample['text'].split()) for sample in ds['train']]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(text_lengths, bins=20, color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"Number of Words in Text\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Text Lengths\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZbsVvpOVmR"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjuZJHZuRqnG"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eovPoZTrNnrp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Basic text preprocessing lowercasing and removing non-alphanumeric characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "ds = ds.map(lambda x: {'cleaned_text': clean_text(x['text'])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5z8Of9JGQaYN"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ntWXL_pQbUw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIpTRzK7RWEq"
      },
      "outputs": [],
      "source": [
        "# Check the first few text samples in the dataset\n",
        "print(ds['train']['text'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3irg4fVdBYy"
      },
      "outputs": [],
      "source": [
        "pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMFbFfk2urbT"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# --- Google Drive Paths ---\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/allergen_detection\"\n",
        "TOKENIZER_PATH = f\"{GDRIVE_PATH}/tokenizer\"\n",
        "MODEL_PATH = f\"{GDRIVE_PATH}/model\"\n",
        "CHECKPOINT_PATH = f\"{GDRIVE_PATH}/checkpoints\"\n",
        "\n",
        "# Create directories if not exists\n",
        "os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(TOKENIZER_PATH, exist_ok=True)\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# --- Configuration ---\n",
        "ALLERGENS = [\n",
        "    # --- Major Regulatory Allergens (FDA/WHO/EU) ---\n",
        "    \"Milk\", \"Eggs\", \"Peanuts\", \"Tree Nuts\", \"Soy\", \"Wheat\",\n",
        "    \"Fish\", \"Shellfish\", \"Sesame\", \"Mustard\", \"Celery\",\n",
        "    \"Lupin\", \"Molluscs\", \"Sulphur Dioxide\", \"Gluten\",\n",
        "\n",
        "    # --- Tree Nuts (Expanded) ---\n",
        "    \"Almonds\", \"Cashews\", \"Pistachios\", \"Walnuts\", \"Pecans\",\n",
        "    \"Hazelnuts\", \"Macadamia\", \"Brazil Nuts\", \"Pine Nuts\",\n",
        "    \"Chestnuts\", \"Beechnuts\", \"Ginkgo Nuts\", \"Hickory Nuts\",\n",
        "    \"Licorice Nuts\", \"Nangai Nuts\", \"Shea Nuts\",\n",
        "\n",
        "    # --- Fish Species ---\n",
        "    \"Anchovy\", \"Basa\", \"Bass\", \"Catfish\", \"Cod\", \"Flounder\",\n",
        "    \"Grouper\", \"Haddock\", \"Hake\", \"Halibut\", \"Herring\", \"Mackerel\",\n",
        "    \"Mahi Mahi\", \"Perch\", \"Pike\", \"Pollock\", \"Salmon\", \"Sardines\",\n",
        "    \"Snapper\", \"Sole\", \"Swordfish\", \"Tilapia\", \"Trout\", \"Tuna\",\n",
        "    \"Whitefish\",\n",
        "\n",
        "    # --- Shellfish/Crustaceans ---\n",
        "    \"Shrimp\", \"Prawns\", \"Crab\", \"Lobster\", \"Crayfish\", \"Scallops\",\n",
        "    \"Clams\", \"Mussels\", \"Oysters\", \"Abalone\", \"Cockles\", \"Whelk\",\n",
        "    \"Periwinkle\", \"Sea Urchin\", \"Barnacle\", \"Krill\", \"Octopus\",\n",
        "    \"Squid\", \"Cuttlefish\",\n",
        "\n",
        "    # --- Grain Allergens ---\n",
        "    \"Barley\", \"Rye\", \"Oats\", \"Spelt\", \"Kamut\", \"Triticale\", \"Durum\",\n",
        "    \"Einkorn\", \"Emmer\", \"Farro\", \"Semolina\", \"Couscous\", \"Bulgur\",\n",
        "    \"Malt\", \"Brewer's Yeast\",\n",
        "\n",
        "    # --- Legume Allergens ---\n",
        "    \"Lentils\", \"Chickpeas\", \"Peas\", \"Green Beans\", \"Lima Beans\",\n",
        "    \"Kidney Beans\", \"Black Beans\", \"Pinto Beans\", \"Navy Beans\",\n",
        "    \"Soybeans\", \"Peanuts\", \"Lupin\", \"Tamari\", \"Edamame\", \"Tempeh\",\n",
        "    \"Natto\", \"Miso\",\n",
        "\n",
        "    # --- Seed Allergens ---\n",
        "    \"Sunflower Seeds\", \"Pumpkin Seeds\", \"Chia Seeds\", \"Flax Seeds\",\n",
        "    \"Hemp Seeds\", \"Poppy Seeds\", \"Cottonseed\", \"Rapeseed\", \"Canola\",\n",
        "\n",
        "    # --- Fruit Allergens ---\n",
        "    \"Kiwi\", \"Mango\", \"Banana\", \"Avocado\", \"Strawberry\", \"Citrus\",\n",
        "    \"Apple\", \"Peach\", \"Cherry\", \"Grape\", \"Melon\", \"Tomato\", \"Papaya\",\n",
        "    \"Pineapple\", \"Passion Fruit\", \"Jackfruit\", \"Lychee\", \"Guava\",\n",
        "\n",
        "    # --- Vegetable Allergens ---\n",
        "    \"Celery\", \"Carrot\", \"Bell Pepper\", \"Chili Pepper\", \"Garlic\",\n",
        "    \"Onion\", \"Asparagus\", \"Broccoli\", \"Cabbage\", \"Cauliflower\",\n",
        "    \"Spinach\", \"Artichoke\", \"Beetroot\", \"Eggplant\", \"Zucchini\",\n",
        "\n",
        "    # --- Spice Allergens ---\n",
        "    \"Cumin\", \"Coriander\", \"Fennel\", \"Paprika\", \"Turmeric\", \"Cinnamon\",\n",
        "    \"Cloves\", \"Nutmeg\", \"Mustard\", \"Pepper\", \"Vanilla\", \"Saffron\",\n",
        "    \"Anise\", \"Cardamom\", \"Sumac\", \"Wasabi\", \"Horseradish\",\n",
        "\n",
        "    # --- Additives/Preservatives ---\n",
        "    \"Sulfites\", \"Benzoates\", \"Nitrates\", \"MSG\", \"Tartrazine\",\n",
        "    \"Carmine\", \"Annatto\", \"Aspartame\", \"Saccharin\", \"BHA/BHT\",\n",
        "    \"Sorbic Acid\", \"Propylene Glycol\", \"Gelatin\", \"Shellac\",\n",
        "    \"Casein\", \"Whey\", \"Lactose\", \"Gluten\",\n",
        "\n",
        "    # --- Regional Allergens ---\n",
        "    \"Buckwheat\", \"Quinoa\", \"Amaranth\", \"Teff\", \"Sorghum\", \"Millet\",\n",
        "    \"Job's Tears\", \"Lingonberry\", \"Cloudberry\", \"Ackee\", \"Breadfruit\",\n",
        "    \"Durian\", \"Rambutan\", \"Salak\", \"Soursop\", \"Star Apple\",\n",
        "\n",
        "    # --- Meat Allergens ---\n",
        "    \"Beef\", \"Pork\", \"Chicken\", \"Turkey\", \"Lamb\", \"Venison\", \"Goat\",\n",
        "    \"Rabbit\", \"Duck\", \"Goose\", \"Kangaroo\", \"Bison\", \"Wild Boar\",\n",
        "\n",
        "    # --- Dairy Derivatives ---\n",
        "    \"Caseinate\", \"Lactalbumin\", \"Lactoglobulin\", \"Ghee\", \"Curd\",\n",
        "    \"Galactose\", \"Dulce de Leche\", \"Paneer\", \"Recaldent\",\n",
        "\n",
        "    # --- Plant-Based Alternatives ---\n",
        "    \"Almond Milk\", \"Soy Milk\", \"Oat Milk\", \"Coconut Milk\", \"Hemp Milk\",\n",
        "    \"Quorn\", \"Seitan\", \"Tofu\", \"Textured Vegetable Protein\",\n",
        "\n",
        "    # --- Insect Derivatives ---\n",
        "    \"Cochineal\", \"Carmine\", \"Shellac\", \"Cricket Flour\", \"Mealworm Protein\",\n",
        "\n",
        "    # --- Alcohol Components ---\n",
        "    \"Histamines\", \"Tannins\", \"Sulfites\", \"Fusel Oils\", \"Peptones\",\n",
        "\n",
        "    # --- Pet Food Cross-Allergens ---\n",
        "    \"Horse Meat\", \"Salmon Meal\", \"Poultry By-Products\", \"Animal Digest\",\n",
        "\n",
        "    # --- Pediatric Allergens ---\n",
        "    \"Cow's Milk Protein\", \"Lactoglobulin\", \"Alpha-Lactalbumin\", \"Bovine Serum Albumin\",\n",
        "\n",
        "    # --- Cross-Reactive Allergens ---\n",
        "    \"Latex-Fruit Proteins\", \"Birch Pollen-Related\", \"Ragweed-Related\",\n",
        "\n",
        "    # --- Emerging Allergens ---\n",
        "    \"Algal Protein\", \"Mung Bean\", \"Water Caltrop\", \"Airborne Yeast\",\n",
        "    \"Microbial Rennet\", \"Lab-Grown Protein\"\n",
        "]\n",
        "\n",
        "\n",
        "ALLERGEN_SYNONYMS = {\n",
        "    # --- Major Regulatory Allergens ---\n",
        "    \"Milk\": [\"casein\", \"whey\", \"lactose\", \"ghee\", \"curd\", \"galactose\", \"recadent\", \"dairy\", \"milk solids\"],\n",
        "    \"Eggs\": [\"ovalbumin\", \"ovomucoid\", \"livetin\", \"albumen\", \"lysozyme\", \"egg white\", \"egg yolk\", \"egg powder\"],\n",
        "    \"Peanuts\": [\"arachis hypogaea\", \"groundnuts\", \"monkey nuts\", \"goobers\", \"beer nuts\", \"earth nuts\"],\n",
        "    \"Tree Nuts\": [\"hard nuts\", \"botanical nuts\", \"culinary nuts\", \"edible seeds\"],\n",
        "    \"Soy\": [\"glycine max\", \"soya\", \"edamame\", \"tempeh\", \"natto\", \"yuba\", \"soy protein\", \"soy lecithin\"],\n",
        "    \"Wheat\": [\"triticum aestivum\", \"bulgur\", \"farina\", \"graham flour\", \"wheat gluten\", \"wheat starch\"],\n",
        "    \"Fish\": [\"pisces\", \"elasmobranchii\", \"osteichthyes\", \"fish oil\", \"fish sauce\", \"fish paste\"],\n",
        "    \"Shellfish\": [\"crustacea\", \"mollusca\", \"cephalopoda\", \"bivalvia\", \"seafood\", \"marine arthropods\"],\n",
        "    \"Sesame\": [\"sesamum indicum\", \"benne seed\", \"gingelly\", \"til\", \"simsim\"],\n",
        "    \"Mustard\": [\"sinapis alba\", \"brassica juncea\", \"yellow powder\", \"mustard oil\", \"mustard greens\"],\n",
        "    \"Celery\": [\"apium graveolens\", \"celeriac\", \"celery salt\", \"celery seed\", \"celery root\"],\n",
        "    \"Lupin\": [\"lupinus\", \"lupine bean\", \"altramuz\", \"lupin flour\", \"lupin protein\"],\n",
        "    \"Molluscs\": [\"gastropoda\", \"bivalvia\", \"cephalopoda\", \"escargot\", \"periwinkle\"],\n",
        "    \"Sulphur Dioxide\": [\"e220\", \"sulfiting agents\", \"sulfur dioxide\", \"sulfites\", \"sulfurous acid\"],\n",
        "    \"Gluten\": [\"gliadin\", \"glutenin\", \"secalin\", \"hordein\", \"wheat protein\"],\n",
        "\n",
        "    # --- Tree Nuts ---\n",
        "    \"Almonds\": [\"prunus dulcis\", \"badam\", \"almond milk\", \"almond oil\", \"marzipan\"],\n",
        "    \"Cashews\": [\"anacardium occidentale\", \"caju\", \"cashew apple\", \"cashew butter\"],\n",
        "    \"Pistachios\": [\"pistacia vera\", \"pista\", \"green almond\", \"smiling nut\"],\n",
        "    \"Walnuts\": [\"juglans regia\", \"akhrot\", \"english walnut\", \"persian walnut\"],\n",
        "    \"Pecans\": [\"carya illinoinensis\", \"pecan pie\", \"pecan oil\", \"native nut\"],\n",
        "    \"Hazelnuts\": [\"corylus\", \"filbert\", \"cobnut\", \"hazel\", \"nutella\"],\n",
        "    \"Macadamia\": [\"macadamia integrifolia\", \"queensland nut\", \"bush nut\", \"bauple nut\"],\n",
        "    \"Brazil Nuts\": [\"bertholletia excelsa\", \"para nuts\", \"amazon nuts\", \"cream nuts\"],\n",
        "    \"Pine Nuts\": [\"pignoli\", \"pi침on\", \"cedar nuts\", \"stone pine seeds\"],\n",
        "    \"Chestnuts\": [\"castanea\", \"ch칙taigne\", \"marron\", \"water chestnut\"],\n",
        "    \"Beechnuts\": [\"fagus\", \"beech mast\", \"beech tree nuts\"],\n",
        "    \"Ginkgo Nuts\": [\"ginkgo biloba\", \"silver apricot\", \"maidenhair tree nut\"],\n",
        "    \"Hickory Nuts\": [\"carya\", \"shagbark\", \"pignut\", \"bitternut\"],\n",
        "    \"Licorice Nuts\": [\"abrus precatorius\", \"jequirity bean\", \"rosary pea\"],\n",
        "    \"Shea Nuts\": [\"vitellaria paradoxa\", \"karite\", \"shea butter\"],\n",
        "\n",
        "    # --- Fish Species ---\n",
        "    \"Anchovy\": [\"engraulis\", \"anchoa\", \"fish sauce\", \"bagoong\"],\n",
        "    \"Basa\": [\"pangasius bocourti\", \"swai\", \"tra\", \"vietnamese catfish\"],\n",
        "    \"Cod\": [\"gadus\", \"scrod\", \"saltfish\", \"bacalao\"],\n",
        "    \"Salmon\": [\"salmo salar\", \"smoked salmon\", \"lox\", \"gravlax\"],\n",
        "    \"Tuna\": [\"thunnus\", \"ahi\", \"albacore\", \"bonito\", \"tonno\"],\n",
        "\n",
        "    # --- Shellfish ---\n",
        "    \"Shrimp\": [\"caridea\", \"prawn\", \"krill\", \"scampi\", \"ebi\"],\n",
        "    \"Crab\": [\"brachyura\", \"soft-shell crab\", \"crab paste\", \"kani\"],\n",
        "    \"Lobster\": [\"homarus\", \"langouste\", \"rock lobster\", \"bisque\"],\n",
        "    \"Octopus\": [\"octopoda\", \"tako\", \"pulpo\", \"moscardino\"],\n",
        "    \"Squid\": [\"teuthida\", \"calamari\", \"ika\", \"chipirones\"],\n",
        "\n",
        "    # --- Grains ---\n",
        "    \"Barley\": [\"hordeum vulgare\", \"malt\", \"barley malt\", \"pearl barley\"],\n",
        "    \"Rye\": [\"secale cereale\", \"rye flour\", \"rye bread\", \"pumpernickel\"],\n",
        "    \"Oats\": [\"avena sativa\", \"oatmeal\", \"rolled oats\", \"oat bran\"],\n",
        "    \"Spelt\": [\"triticum spelta\", \"dinkel\", \"farro grande\", \"hulled wheat\"],\n",
        "\n",
        "    # --- Legumes ---\n",
        "    \"Lentils\": [\"lens culinaris\", \"masoor\", \"red lentil\", \"puy lentil\"],\n",
        "    \"Chickpeas\": [\"cicer arietinum\", \"garbanzo\", \"chana\", \"hummus\"],\n",
        "    \"Peas\": [\"pisum sativum\", \"green peas\", \"split peas\", \"mangetout\"],\n",
        "\n",
        "\n",
        "    # --- Additives ---\n",
        "    \"MSG\": [\"monosodium glutamate\", \"e621\", \"ajinomoto\", \"umami seasoning\"],\n",
        "    \"Carmine\": [\"cochineal\", \"e120\", \"natural red 4\", \"carminic acid\"],\n",
        "    \"Aspartame\": [\"nutrasweet\", \"e951\", \"equal\", \"canderel\"],\n",
        "\n",
        "    # --- Regional Allergens ---\n",
        "    \"Durian\": [\"king of fruits\", \"civet fruit\", \"stink fruit\"],\n",
        "    \"Ackee\": [\"blighia sapida\", \"akee\", \"vegetable brain\", \"national fruit jamaica\"],\n",
        "\n",
        "    # --- Emerging Allergens ---\n",
        "    \"Cricket Flour\": [\"insect protein\", \"orthoptera powder\", \"entomophagy product\"],\n",
        "    \"Lab-Grown Protein\": [\"cultured meat\", \"in vitro protein\", \"cellular agriculture\"]\n",
        "}\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def generate_labels(text):\n",
        "    \"\"\"Efficient label generation using existing text\"\"\"\n",
        "    text = text.lower()\n",
        "    return [\n",
        "        # Convert to float32 for PyTorch compatibility\n",
        "        float(any(syn in text for syn in [allergen.lower()] + ALLERGEN_SYNONYMS.get(allergen, [])))\n",
        "        for allergen in ALLERGENS\n",
        "    ]\n",
        "\n",
        "# --- Dataset Processing ---\n",
        "def process_dataset(dataset):\n",
        "    \"\"\"Process dataset using existing text fields\"\"\"\n",
        "    return Dataset.from_dict({\n",
        "        \"text\": [ex[\"text\"] for ex in dataset],\n",
        "        # Convert labels to float32\n",
        "        \"labels\": [generate_labels(ex[\"text\"]) for ex in dataset]\n",
        "    }).filter(lambda x: x[\"text\"] != \"\").with_format(\"torch\")\n",
        "\n",
        "# --- Main Workflow ---\n",
        "def main():\n",
        "    # 1. Load Dataset\n",
        "    dataset = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")\n",
        "\n",
        "    # 2. Process Data\n",
        "    processed_ds = process_dataset(dataset[\"train\"])\n",
        "\n",
        "    # 3. Train/Test Split\n",
        "    split_ds = processed_ds.train_test_split(test_size=0.2)\n",
        "\n",
        "    # 4. Tokenizer Setup\n",
        "    if os.listdir(TOKENIZER_PATH):\n",
        "        tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "    else:\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        tokenizer.save_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "    # 5. Tokenization with proper dtype\n",
        "    def tokenize_fn(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    tokenized_ds = split_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "    # 6. Model Initialization with correct dtype\n",
        "    if os.listdir(CHECKPOINT_PATH):\n",
        "        model = BertForSequenceClassification.from_pretrained(CHECKPOINT_PATH)\n",
        "    else:\n",
        "        model = BertForSequenceClassification.from_pretrained(\n",
        "            \"bert-base-uncased\",\n",
        "            num_labels=len(ALLERGENS),\n",
        "            problem_type=\"multi_label_classification\",\n",
        "            torch_dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    # 7. Training Setup with correct types\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=CHECKPOINT_PATH,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{GDRIVE_PATH}/logs\",\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=True,\n",
        "        optim=\"adamw_torch\",\n",
        "        fp16=False  # Disable mixed precision for type stability\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_ds[\"train\"],\n",
        "        eval_dataset=tokenized_ds[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # 8. Train with Cleanup\n",
        "    try:\n",
        "        trainer.train(resume_from_checkpoint=bool(os.listdir(CHECKPOINT_PATH)))\n",
        "    finally:\n",
        "        # Save final model\n",
        "        trainer.save_model(MODEL_PATH)\n",
        "        tokenizer.save_pretrained(MODEL_PATH)\n",
        "\n",
        "        # Cleanup checkpoints\n",
        "        shutil.rmtree(CHECKPOINT_PATH)\n",
        "        os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "        # Free memory\n",
        "        del model, trainer, tokenizer, tokenized_ds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# --- Inference Function ---\n",
        "def predict_from_text(text):\n",
        "    \"\"\"Direct prediction from text input\"\"\"\n",
        "    # Load from Google Drive\n",
        "    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    # Predict with proper dtype\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Cleanup\n",
        "    del model, tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return [\n",
        "        (ALLERGENS[i], float(torch.sigmoid(outputs.logits)[0][i]))\n",
        "        for i in torch.topk(outputs.logits, 5).indices[0].tolist()\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLu6k3OPx6Hx"
      },
      "outputs": [],
      "source": [
        "# Standalone Evaluation Code with Drive Persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset, load_from_disk\n",
        "\n",
        "# --- Configuration ---\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/allergen_detection\"\n",
        "MODEL_PATH = f\"{GDRIVE_PATH}/model\"\n",
        "DATA_PATH = f\"{GDRIVE_PATH}/evaluation_data\"\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# --- Persistent Storage Files ---\n",
        "TOKENIZED_PATH = f\"{DATA_PATH}/tokenized\"\n",
        "LABELS_PATH = f\"{DATA_PATH}/labels.pt\"\n",
        "ALLERGENS_PATH = f\"{DATA_PATH}/allergens.txt\"\n",
        "SYNONYMS_PATH = f\"{DATA_PATH}/synonyms.pt\"\n",
        "\n",
        "def save_components():\n",
        "    \"\"\"Save labels and synonyms to Drive\"\"\"\n",
        "    torch.save({\n",
        "        'ALLERGENS': ALLERGENS,\n",
        "        'ALLERGEN_SYNONYMS': ALLERGEN_SYNONYMS\n",
        "    }, SYNONYMS_PATH)\n",
        "\n",
        "    with open(ALLERGENS_PATH, \"w\") as f:\n",
        "        f.write(\"\\n\".join(ALLERGENS))\n",
        "\n",
        "def load_components():\n",
        "    \"\"\"Load components from Drive or create new\"\"\"\n",
        "    if os.path.exists(SYNONYMS_PATH):\n",
        "        data = torch.load(SYNONYMS_PATH)\n",
        "        return data['ALLERGENS'], data['ALLERGEN_SYNONYMS']\n",
        "\n",
        "    # Define your original lists here\n",
        "    ALLERGENS = [\n",
        "        # --- Major Regulatory Allergens (FDA/WHO/EU) ---\n",
        "    \"Milk\", \"Eggs\", \"Peanuts\", \"Tree Nuts\", \"Soy\", \"Wheat\",\n",
        "    \"Fish\", \"Shellfish\", \"Sesame\", \"Mustard\", \"Celery\",\n",
        "    \"Lupin\", \"Molluscs\", \"Sulphur Dioxide\", \"Gluten\",\n",
        "\n",
        "    # --- Tree Nuts (Expanded) ---\n",
        "    \"Almonds\", \"Cashews\", \"Pistachios\", \"Walnuts\", \"Pecans\",\n",
        "    \"Hazelnuts\", \"Macadamia\", \"Brazil Nuts\", \"Pine Nuts\",\n",
        "    \"Chestnuts\", \"Beechnuts\", \"Ginkgo Nuts\", \"Hickory Nuts\",\n",
        "    \"Licorice Nuts\", \"Nangai Nuts\", \"Shea Nuts\",\n",
        "\n",
        "    # --- Fish Species ---\n",
        "    \"Anchovy\", \"Basa\", \"Bass\", \"Catfish\", \"Cod\", \"Flounder\",\n",
        "    \"Grouper\", \"Haddock\", \"Hake\", \"Halibut\", \"Herring\", \"Mackerel\",\n",
        "    \"Mahi Mahi\", \"Perch\", \"Pike\", \"Pollock\", \"Salmon\", \"Sardines\",\n",
        "    \"Snapper\", \"Sole\", \"Swordfish\", \"Tilapia\", \"Trout\", \"Tuna\",\n",
        "    \"Whitefish\",\n",
        "\n",
        "    # --- Shellfish/Crustaceans ---\n",
        "    \"Shrimp\", \"Prawns\", \"Crab\", \"Lobster\", \"Crayfish\", \"Scallops\",\n",
        "    \"Clams\", \"Mussels\", \"Oysters\", \"Abalone\", \"Cockles\", \"Whelk\",\n",
        "    \"Periwinkle\", \"Sea Urchin\", \"Barnacle\", \"Krill\", \"Octopus\",\n",
        "    \"Squid\", \"Cuttlefish\",\n",
        "\n",
        "    # --- Grain Allergens ---\n",
        "    \"Barley\", \"Rye\", \"Oats\", \"Spelt\", \"Kamut\", \"Triticale\", \"Durum\",\n",
        "    \"Einkorn\", \"Emmer\", \"Farro\", \"Semolina\", \"Couscous\", \"Bulgur\",\n",
        "    \"Malt\", \"Brewer's Yeast\",\n",
        "\n",
        "    # --- Legume Allergens ---\n",
        "    \"Lentils\", \"Chickpeas\", \"Peas\", \"Green Beans\", \"Lima Beans\",\n",
        "    \"Kidney Beans\", \"Black Beans\", \"Pinto Beans\", \"Navy Beans\",\n",
        "    \"Soybeans\", \"Peanuts\", \"Lupin\", \"Tamari\", \"Edamame\", \"Tempeh\",\n",
        "    \"Natto\", \"Miso\",\n",
        "\n",
        "    # --- Seed Allergens ---\n",
        "    \"Sunflower Seeds\", \"Pumpkin Seeds\", \"Chia Seeds\", \"Flax Seeds\",\n",
        "    \"Hemp Seeds\", \"Poppy Seeds\", \"Cottonseed\", \"Rapeseed\", \"Canola\",\n",
        "\n",
        "    # --- Fruit Allergens ---\n",
        "    \"Kiwi\", \"Mango\", \"Banana\", \"Avocado\", \"Strawberry\", \"Citrus\",\n",
        "    \"Apple\", \"Peach\", \"Cherry\", \"Grape\", \"Melon\", \"Tomato\", \"Papaya\",\n",
        "    \"Pineapple\", \"Passion Fruit\", \"Jackfruit\", \"Lychee\", \"Guava\",\n",
        "\n",
        "    # --- Vegetable Allergens ---\n",
        "    \"Celery\", \"Carrot\", \"Bell Pepper\", \"Chili Pepper\", \"Garlic\",\n",
        "    \"Onion\", \"Asparagus\", \"Broccoli\", \"Cabbage\", \"Cauliflower\",\n",
        "    \"Spinach\", \"Artichoke\", \"Beetroot\", \"Eggplant\", \"Zucchini\",\n",
        "\n",
        "    # --- Spice Allergens ---\n",
        "    \"Cumin\", \"Coriander\", \"Fennel\", \"Paprika\", \"Turmeric\", \"Cinnamon\",\n",
        "    \"Cloves\", \"Nutmeg\", \"Mustard\", \"Pepper\", \"Vanilla\", \"Saffron\",\n",
        "    \"Anise\", \"Cardamom\", \"Sumac\", \"Wasabi\", \"Horseradish\",\n",
        "\n",
        "    # --- Additives/Preservatives ---\n",
        "    \"Sulfites\", \"Benzoates\", \"Nitrates\", \"MSG\", \"Tartrazine\",\n",
        "    \"Carmine\", \"Annatto\", \"Aspartame\", \"Saccharin\", \"BHA/BHT\",\n",
        "    \"Sorbic Acid\", \"Propylene Glycol\", \"Gelatin\", \"Shellac\",\n",
        "    \"Casein\", \"Whey\", \"Lactose\", \"Gluten\",\n",
        "\n",
        "    # --- Regional Allergens ---\n",
        "    \"Buckwheat\", \"Quinoa\", \"Amaranth\", \"Teff\", \"Sorghum\", \"Millet\",\n",
        "    \"Job's Tears\", \"Lingonberry\", \"Cloudberry\", \"Ackee\", \"Breadfruit\",\n",
        "    \"Durian\", \"Rambutan\", \"Salak\", \"Soursop\", \"Star Apple\",\n",
        "\n",
        "    # --- Meat Allergens ---\n",
        "    \"Beef\", \"Pork\", \"Chicken\", \"Turkey\", \"Lamb\", \"Venison\", \"Goat\",\n",
        "    \"Rabbit\", \"Duck\", \"Goose\", \"Kangaroo\", \"Bison\", \"Wild Boar\",\n",
        "\n",
        "    # --- Dairy Derivatives ---\n",
        "    \"Caseinate\", \"Lactalbumin\", \"Lactoglobulin\", \"Ghee\", \"Curd\",\n",
        "    \"Galactose\", \"Dulce de Leche\", \"Paneer\", \"Recaldent\",\n",
        "\n",
        "    # --- Plant-Based Alternatives ---\n",
        "    \"Almond Milk\", \"Soy Milk\", \"Oat Milk\", \"Coconut Milk\", \"Hemp Milk\",\n",
        "    \"Quorn\", \"Seitan\", \"Tofu\", \"Textured Vegetable Protein\",\n",
        "\n",
        "    # --- Insect Derivatives ---\n",
        "    \"Cochineal\", \"Carmine\", \"Shellac\", \"Cricket Flour\", \"Mealworm Protein\",\n",
        "\n",
        "    # --- Alcohol Components ---\n",
        "    \"Histamines\", \"Tannins\", \"Sulfites\", \"Fusel Oils\", \"Peptones\",\n",
        "\n",
        "    # --- Pet Food Cross-Allergens ---\n",
        "    \"Horse Meat\", \"Salmon Meal\", \"Poultry By-Products\", \"Animal Digest\",\n",
        "\n",
        "    # --- Pediatric Allergens ---\n",
        "    \"Cow's Milk Protein\", \"Lactoglobulin\", \"Alpha-Lactalbumin\", \"Bovine Serum Albumin\",\n",
        "\n",
        "    # --- Cross-Reactive Allergens ---\n",
        "    \"Latex-Fruit Proteins\", \"Birch Pollen-Related\", \"Ragweed-Related\",\n",
        "\n",
        "    # --- Emerging Allergens ---\n",
        "    \"Algal Protein\", \"Mung Bean\", \"Water Caltrop\", \"Airborne Yeast\",\n",
        "    \"Microbial Rennet\", \"Lab-Grown Protein\"\n",
        "    ]\n",
        "\n",
        "    ALLERGEN_SYNONYMS = {\n",
        "        # --- Major Regulatory Allergens ---\n",
        "    \"Milk\": [\"casein\", \"whey\", \"lactose\", \"ghee\", \"curd\", \"galactose\", \"recadent\", \"dairy\", \"milk solids\"],\n",
        "    \"Eggs\": [\"ovalbumin\", \"ovomucoid\", \"livetin\", \"albumen\", \"lysozyme\", \"egg white\", \"egg yolk\", \"egg powder\"],\n",
        "    \"Peanuts\": [\"arachis hypogaea\", \"groundnuts\", \"monkey nuts\", \"goobers\", \"beer nuts\", \"earth nuts\"],\n",
        "    \"Tree Nuts\": [\"hard nuts\", \"botanical nuts\", \"culinary nuts\", \"edible seeds\"],\n",
        "    \"Soy\": [\"glycine max\", \"soya\", \"edamame\", \"tempeh\", \"natto\", \"yuba\", \"soy protein\", \"soy lecithin\"],\n",
        "    \"Wheat\": [\"triticum aestivum\", \"bulgur\", \"farina\", \"graham flour\", \"wheat gluten\", \"wheat starch\"],\n",
        "    \"Fish\": [\"pisces\", \"elasmobranchii\", \"osteichthyes\", \"fish oil\", \"fish sauce\", \"fish paste\"],\n",
        "    \"Shellfish\": [\"crustacea\", \"mollusca\", \"cephalopoda\", \"bivalvia\", \"seafood\", \"marine arthropods\"],\n",
        "    \"Sesame\": [\"sesamum indicum\", \"benne seed\", \"gingelly\", \"til\", \"simsim\"],\n",
        "    \"Mustard\": [\"sinapis alba\", \"brassica juncea\", \"yellow powder\", \"mustard oil\", \"mustard greens\"],\n",
        "    \"Celery\": [\"apium graveolens\", \"celeriac\", \"celery salt\", \"celery seed\", \"celery root\"],\n",
        "    \"Lupin\": [\"lupinus\", \"lupine bean\", \"altramuz\", \"lupin flour\", \"lupin protein\"],\n",
        "    \"Molluscs\": [\"gastropoda\", \"bivalvia\", \"cephalopoda\", \"escargot\", \"periwinkle\"],\n",
        "    \"Sulphur Dioxide\": [\"e220\", \"sulfiting agents\", \"sulfur dioxide\", \"sulfites\", \"sulfurous acid\"],\n",
        "    \"Gluten\": [\"gliadin\", \"glutenin\", \"secalin\", \"hordein\", \"wheat protein\"],\n",
        "\n",
        "    # --- Tree Nuts ---\n",
        "    \"Almonds\": [\"prunus dulcis\", \"badam\", \"almond milk\", \"almond oil\", \"marzipan\"],\n",
        "    \"Cashews\": [\"anacardium occidentale\", \"caju\", \"cashew apple\", \"cashew butter\"],\n",
        "    \"Pistachios\": [\"pistacia vera\", \"pista\", \"green almond\", \"smiling nut\"],\n",
        "    \"Walnuts\": [\"juglans regia\", \"akhrot\", \"english walnut\", \"persian walnut\"],\n",
        "    \"Pecans\": [\"carya illinoinensis\", \"pecan pie\", \"pecan oil\", \"native nut\"],\n",
        "    \"Hazelnuts\": [\"corylus\", \"filbert\", \"cobnut\", \"hazel\", \"nutella\"],\n",
        "    \"Macadamia\": [\"macadamia integrifolia\", \"queensland nut\", \"bush nut\", \"bauple nut\"],\n",
        "    \"Brazil Nuts\": [\"bertholletia excelsa\", \"para nuts\", \"amazon nuts\", \"cream nuts\"],\n",
        "    \"Pine Nuts\": [\"pignoli\", \"pi침on\", \"cedar nuts\", \"stone pine seeds\"],\n",
        "    \"Chestnuts\": [\"castanea\", \"ch칙taigne\", \"marron\", \"water chestnut\"],\n",
        "    \"Beechnuts\": [\"fagus\", \"beech mast\", \"beech tree nuts\"],\n",
        "    \"Ginkgo Nuts\": [\"ginkgo biloba\", \"silver apricot\", \"maidenhair tree nut\"],\n",
        "    \"Hickory Nuts\": [\"carya\", \"shagbark\", \"pignut\", \"bitternut\"],\n",
        "    \"Licorice Nuts\": [\"abrus precatorius\", \"jequirity bean\", \"rosary pea\"],\n",
        "    \"Shea Nuts\": [\"vitellaria paradoxa\", \"karite\", \"shea butter\"],\n",
        "\n",
        "    # --- Fish Species ---\n",
        "    \"Anchovy\": [\"engraulis\", \"anchoa\", \"fish sauce\", \"bagoong\"],\n",
        "    \"Basa\": [\"pangasius bocourti\", \"swai\", \"tra\", \"vietnamese catfish\"],\n",
        "    \"Cod\": [\"gadus\", \"scrod\", \"saltfish\", \"bacalao\"],\n",
        "    \"Salmon\": [\"salmo salar\", \"smoked salmon\", \"lox\", \"gravlax\"],\n",
        "    \"Tuna\": [\"thunnus\", \"ahi\", \"albacore\", \"bonito\", \"tonno\"],\n",
        "\n",
        "    # --- Shellfish ---\n",
        "    \"Shrimp\": [\"caridea\", \"prawn\", \"krill\", \"scampi\", \"ebi\"],\n",
        "    \"Crab\": [\"brachyura\", \"soft-shell crab\", \"crab paste\", \"kani\"],\n",
        "    \"Lobster\": [\"homarus\", \"langouste\", \"rock lobster\", \"bisque\"],\n",
        "    \"Octopus\": [\"octopoda\", \"tako\", \"pulpo\", \"moscardino\"],\n",
        "    \"Squid\": [\"teuthida\", \"calamari\", \"ika\", \"chipirones\"],\n",
        "\n",
        "    # --- Grains ---\n",
        "    \"Barley\": [\"hordeum vulgare\", \"malt\", \"barley malt\", \"pearl barley\"],\n",
        "    \"Rye\": [\"secale cereale\", \"rye flour\", \"rye bread\", \"pumpernickel\"],\n",
        "    \"Oats\": [\"avena sativa\", \"oatmeal\", \"rolled oats\", \"oat bran\"],\n",
        "    \"Spelt\": [\"triticum spelta\", \"dinkel\", \"farro grande\", \"hulled wheat\"],\n",
        "\n",
        "    # --- Legumes ---\n",
        "    \"Lentils\": [\"lens culinaris\", \"masoor\", \"red lentil\", \"puy lentil\"],\n",
        "    \"Chickpeas\": [\"cicer arietinum\", \"garbanzo\", \"chana\", \"hummus\"],\n",
        "    \"Peas\": [\"pisum sativum\", \"green peas\", \"split peas\", \"mangetout\"],\n",
        "\n",
        "\n",
        "    # --- Additives ---\n",
        "    \"MSG\": [\"monosodium glutamate\", \"e621\", \"ajinomoto\", \"umami seasoning\"],\n",
        "    \"Carmine\": [\"cochineal\", \"e120\", \"natural red 4\", \"carminic acid\"],\n",
        "    \"Aspartame\": [\"nutrasweet\", \"e951\", \"equal\", \"canderel\"],\n",
        "\n",
        "    # --- Regional Allergens ---\n",
        "    \"Durian\": [\"king of fruits\", \"civet fruit\", \"stink fruit\"],\n",
        "    \"Ackee\": [\"blighia sapida\", \"akee\", \"vegetable brain\", \"national fruit jamaica\"],\n",
        "\n",
        "    # --- Emerging Allergens ---\n",
        "    \"Cricket Flour\": [\"insect protein\", \"orthoptera powder\", \"entomophagy product\"],\n",
        "    \"Lab-Grown Protein\": [\"cultured meat\", \"in vitro protein\", \"cellular agriculture\"]\n",
        "    }\n",
        "\n",
        "    # Save for future sessions\n",
        "    save_components()\n",
        "    return ALLERGENS, ALLERGEN_SYNONYMS\n",
        "\n",
        "def get_tokenized_data(tokenizer):\n",
        "    \"\"\"Load or create tokenized dataset\"\"\"\n",
        "    if os.path.exists(TOKENIZED_PATH):\n",
        "        return load_from_disk(TOKENIZED_PATH)\n",
        "\n",
        "    # Process and save if not exists\n",
        "    raw_data = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")[\"train\"]\n",
        "\n",
        "    # Create dataset with existing text\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"text\": [ex[\"text\"] for ex in raw_data],\n",
        "        \"labels\": [generate_labels(ex[\"text\"]) for ex in raw_data]\n",
        "    }).filter(lambda x: x[\"text\"] != \"\")\n",
        "\n",
        "    split_ds = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "    # Tokenize\n",
        "    def tokenize_fn(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    tokenized = split_ds.map(tokenize_fn, batched=True)\n",
        "    tokenized.save_to_disk(TOKENIZED_PATH)\n",
        "    return tokenized\n",
        "\n",
        "def generate_classification_report():\n",
        "    \"\"\"Generate and save evaluation report\"\"\"\n",
        "    # Load components\n",
        "    ALLERGENS, ALLERGEN_SYNONYMS = load_components()\n",
        "    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    # Get tokenized data\n",
        "    tokenized_ds = get_tokenized_data(tokenizer)\n",
        "\n",
        "    # Create trainer without logging\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(\n",
        "            output_dir=\"./tmp\",\n",
        "            per_device_eval_batch_size=16,\n",
        "            disable_tqdm=True,\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    predictions = trainer.predict(tokenized_ds[\"test\"])\n",
        "    probs = torch.sigmoid(torch.Tensor(predictions.predictions))\n",
        "    y_pred = (probs > 0.3).long().numpy()\n",
        "\n",
        "    # Generate and save report\n",
        "    report = classification_report(\n",
        "        tokenized_ds[\"test\"][\"labels\"],\n",
        "        y_pred,\n",
        "        target_names=ALLERGENS,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    with open(f\"{DATA_PATH}/classification_report.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(report)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_classification_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKIOhKuP1gf6"
      },
      "outputs": [],
      "source": [
        "# Check label distribution\n",
        "import numpy as np\n",
        "from datasets import load_from_disk  # Import for loading from disk\n",
        "from transformers import BertTokenizer # Import BertTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/allergen_detection/model\")\n",
        "TOKENIZED_PATH = \"/content/drive/MyDrive/allergen_detection/evaluation_data/tokenized\"\n",
        "\n",
        "# Load the tokenized dataset from disk\n",
        "tokenized_ds = load_from_disk(TOKENIZED_PATH)\n",
        "\n",
        "# Access true labels from the test set\n",
        "y_true = np.array(tokenized_ds[\"test\"][\"labels\"])\n",
        "\n",
        "label_counts = np.sum(y_true, axis=0)\n",
        "\n",
        "# Load allergens\n",
        "try:\n",
        "  ALLERGENS\n",
        "except NameError:\n",
        "  with open(\"/content/drive/MyDrive/allergen_detection/evaluation_data/allergens.txt\", \"r\") as f:\n",
        "    ALLERGENS = f.read().splitlines()\n",
        "\n",
        "for allergen, count in zip(ALLERGENS, label_counts):\n",
        "    if count > 0:\n",
        "        print(f\"{allergen}: {count} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW-zxDPLNIYv"
      },
      "outputs": [],
      "source": [
        "# Class Consolidation and Weighted Training\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- Configuration ---\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/allergen_detection\"\n",
        "MODEL_PATH = f\"{GDRIVE_PATH}/model\"\n",
        "CHECKPOINT_PATH = f\"{GDRIVE_PATH}/checkpoints\"\n",
        "\n",
        "# --- Class Consolidation Mapping ---\n",
        "CONSOLIDATION_MAP = {\n",
        "    \"Tree Nuts\": [\n",
        "        \"Almonds\", \"Cashews\", \"Pistachios\", \"Walnuts\", \"Pecans\",\n",
        "        \"Hazelnuts\", \"Macadamia\", \"Brazil Nuts\", \"Pine Nuts\", \"Chestnuts\"\n",
        "    ],\n",
        "    \"Shellfish and Molluscs\": [\"Shrimp\", \"Crab\", \"Squid\", \"Cuttlefish\", \"Molluscs\", \"Lobster\"],\n",
        "    \"Fish\": [\n",
        "        \"Anchovy\", \"Basa\", \"Bass\", \"Cod\", \"Hake\", \"Perch\",\n",
        "        \"Pollock\", \"Salmon\", \"Sole\", \"Tuna\"\n",
        "    ],\n",
        "    \"Grains\": [\n",
        "        \"Barley\", \"Rye\", \"Oats\", \"Spelt\", \"Triticale\", \"Durum\",\n",
        "        \"Emmer\", \"Farro\", \"Semolina\", \"Couscous\", \"Bulgur\", \"Malt\"\n",
        "    ],\n",
        "    \"Legumes\": [\n",
        "        \"Lentils\", \"Chickpeas\", \"Peas\", \"Lima Beans\", \"Kidney Beans\",\n",
        "        \"Black Beans\", \"Pinto Beans\", \"Navy Beans\", \"Soybeans\"\n",
        "    ],\n",
        "    \"Dairy Derivatives\": [\"Casein\", \"Whey\", \"Lactose\", \"Caseinate\", \"Paneer\"],\n",
        "    \"Seeds\": [\n",
        "        \"Sunflower Seeds\", \"Pumpkin Seeds\", \"Chia Seeds\",\n",
        "        \"Cottonseed\", \"Rapeseed\", \"Canola\"\n",
        "    ],\n",
        "    \"Fruits\": [\n",
        "        \"Kiwi\", \"Mango\", \"Banana\", \"Avocado\", \"Strawberry\", \"Citrus\",\n",
        "        \"Apple\", \"Peach\", \"Cherry\", \"Grape\", \"Melon\", \"Tomato\",\n",
        "        \"Papaya\", \"Pineapple\", \"Passion Fruit\"\n",
        "    ],\n",
        "    \"Vegetables\": [\n",
        "        \"Celery\", \"Carrot\", \"Bell Pepper\", \"Chili Pepper\", \"Garlic\",\n",
        "        \"Onion\", \"Broccoli\", \"Cabbage\", \"Cauliflower\", \"Spinach\",\n",
        "        \"Beetroot\", \"Zucchini\"\n",
        "    ],\n",
        "    \"Spices\": [\n",
        "        \"Cumin\", \"Coriander\", \"Fennel\", \"Paprika\", \"Turmeric\",\n",
        "        \"Cinnamon\", \"Cloves\", \"Nutmeg\", \"Mustard\", \"Pepper\",\n",
        "        \"Vanilla\", \"Anise\", \"Cardamom\", \"Wasabi\", \"Horseradish\"\n",
        "    ],\n",
        "    \"Food Additives\": [\n",
        "        \"Sulfites\", \"Nitrates\", \"MSG\", \"Tartrazine\", \"Carmine\",\n",
        "        \"Annatto\", \"Aspartame\", \"Saccharin\", \"Sorbic Acid\",\n",
        "        \"Propylene Glycol\", \"Cochineal\", \"Shellac\"\n",
        "    ],\n",
        "    \"Alternative Grains\": [\n",
        "        \"Buckwheat\", \"Quinoa\", \"Amaranth\", \"Sorghum\", \"Millet\"\n",
        "    ],\n",
        "    \"Animal Proteins\": [\n",
        "        \"Beef\", \"Pork\", \"Chicken\", \"Turkey\", \"Lamb\",\n",
        "        \"Venison\", \"Goat\", \"Duck\", \"Gelatin\"\n",
        "    ],\n",
        "    \"Plant Proteins\": [\"Soy Milk\", \"Coconut Milk\", \"Seitan\", \"Tofu\"]\n",
        "}\n",
        "\n",
        "KEEP_SEPARATE = [\n",
        "    \"Eggs\", \"Peanuts\", \"Soy\", \"Wheat\", \"Sesame\", \"Gluten\", \"Sulphur Dioxide\"\n",
        "]\n",
        "\n",
        "# Load original allergens\n",
        "with open(f\"{GDRIVE_PATH}/evaluation_data/allergens.txt\", \"r\") as f:\n",
        "    original_allergens = f.read().splitlines()\n",
        "\n",
        "# Create consolidated list\n",
        "CONSOLIDATED_ALLERGENS = [\n",
        "    cls for cls in original_allergens\n",
        "    if not any(cls in v for v in CONSOLIDATION_MAP.values())\n",
        "] + list(CONSOLIDATION_MAP.keys())\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def generate_labels(text, allergens=None):\n",
        "    \"\"\"\n",
        "    Generate binary labels for allergens in text\n",
        "    \"\"\"\n",
        "    if allergens is None:\n",
        "        allergens = original_allergens\n",
        "\n",
        "    # Convert text to lowercase for case-insensitive matching\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Create binary vector\n",
        "    labels = np.zeros(len(allergens), dtype=np.float32)\n",
        "\n",
        "    # Mark allergens that appear in the text\n",
        "    for i, allergen in enumerate(allergens):\n",
        "        if allergen.lower() in text_lower:\n",
        "            labels[i] = 1.0\n",
        "\n",
        "    return labels\n",
        "\n",
        "def consolidate_labels(labels, original_classes, consolidation_map):\n",
        "    \"\"\"Convert original labels to consolidated labels\"\"\"\n",
        "    # Get the list of consolidated classes\n",
        "    consolidated_classes = [\n",
        "        cls for cls in original_classes\n",
        "        if not any(cls in v for v in consolidation_map.values())\n",
        "    ] + list(consolidation_map.keys())\n",
        "\n",
        "    # Create empty array for consolidated labels\n",
        "    consolidated = np.zeros((len(labels), len(consolidated_classes)), dtype=np.float32)\n",
        "\n",
        "    # Create mapping from original to consolidated indices\n",
        "    class_mapping = {}\n",
        "    for i, cls in enumerate(original_classes):\n",
        "        # Check if this class is part of a consolidated group\n",
        "        for group_name, members in consolidation_map.items():\n",
        "            if cls in members:\n",
        "                # Find the index of the group name in consolidated_classes\n",
        "                group_idx = consolidated_classes.index(group_name)\n",
        "                class_mapping[i] = group_idx\n",
        "                break\n",
        "        else:\n",
        "            # If not part of a group, find its original position in the consolidated list\n",
        "            if cls in consolidated_classes:\n",
        "                class_mapping[i] = consolidated_classes.index(cls)\n",
        "            else:\n",
        "                # Skip classes that are not in the consolidated list\n",
        "                continue\n",
        "\n",
        "    # Aggregate labels\n",
        "    for orig_idx, cons_idx in class_mapping.items():\n",
        "        consolidated[:, cons_idx] = np.logical_or(\n",
        "            consolidated[:, cons_idx],\n",
        "            labels[:, orig_idx]\n",
        "        )\n",
        "\n",
        "    return consolidated\n",
        "\n",
        "# --- Dataset Processing ---\n",
        "def process_dataset(dataset):\n",
        "    \"\"\"Process dataset with consolidated labels\"\"\"\n",
        "    # Generate original labels\n",
        "    original_labels = [generate_labels(ex[\"text\"]) for ex in dataset]\n",
        "\n",
        "    # Consolidate labels\n",
        "    consolidated = consolidate_labels(\n",
        "        np.array(original_labels),\n",
        "        original_allergens,\n",
        "        CONSOLIDATION_MAP\n",
        "    )\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"text\": [ex[\"text\"] for ex in dataset],\n",
        "        \"labels\": consolidated\n",
        "    }).filter(lambda x: x[\"text\"] != \"\").with_format(\"torch\")\n",
        "\n",
        "# --- Class Weighting ---\n",
        "def get_class_weights(dataset):\n",
        "    \"\"\"Calculate balanced class weights using a binary approach for each class\"\"\"\n",
        "    y_train = np.array(dataset[\"train\"][\"labels\"])\n",
        "    num_classes = y_train.shape[1]\n",
        "    weights = np.ones(num_classes, dtype=np.float32)\n",
        "\n",
        "    # Calculate weight for each class independently\n",
        "    for i in range(num_classes):\n",
        "        # Extract binary labels for this class (0 or 1)\n",
        "        class_labels = y_train[:, i]\n",
        "\n",
        "        # Only compute weights if contains both positive and negative examples\n",
        "        if np.any(class_labels == 1) and np.any(class_labels == 0):\n",
        "            # Use compute_class_weight with binary classes (0, 1)\n",
        "            class_weights = compute_class_weight(\n",
        "                'balanced',\n",
        "                classes=np.array([0, 1]),\n",
        "                y=class_labels\n",
        "            )\n",
        "            # Store only the weight for positive class (index 1)\n",
        "            weights[i] = class_weights[1]\n",
        "\n",
        "    print(f\"Class weights range: min={weights.min():.2f}, max={weights.max():.2f}\")\n",
        "    return weights\n",
        "\n",
        "# --- Custom Loss Function ---\n",
        "class WeightedBCELoss(torch.nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super().__init__()\n",
        "        self.weights = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, reduction='none'\n",
        "        )\n",
        "        # Apply weights along the correct dimension\n",
        "        weight_tensor = self.weights.to(inputs.device).view(1, -1)\n",
        "        weighted_loss = loss * weight_tensor\n",
        "        return weighted_loss.mean()\n",
        "\n",
        "# --- Main Workflow ---\n",
        "def main():\n",
        "    # Load and process dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")\n",
        "\n",
        "    print(\"Processing dataset with consolidated labels...\")\n",
        "    processed_ds = process_dataset(dataset[\"train\"])\n",
        "    split_ds = processed_ds.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    print(f\"Number of consolidated classes: {len(CONSOLIDATED_ALLERGENS)}\")\n",
        "    print(f\"Train set size: {len(split_ds['train'])}\")\n",
        "    print(f\"Test set size: {len(split_ds['test'])}\")\n",
        "\n",
        "    # Calculate class weights\n",
        "    print(\"Calculating class weights...\")\n",
        "    class_weights = get_class_weights(split_ds)\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=len(CONSOLIDATED_ALLERGENS),\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    # Tokenization\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    def tokenize_fn(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    tokenized_ds = split_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "    # Create custom trainer class that uses loss function\n",
        "    class CustomTrainer(Trainer):\n",
        "       def __init__(self, loss_function=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = self.loss_function(logits, labels) if self.loss_function else None\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "    # Training setup\n",
        "    print(\"Setting up training arguments...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=CHECKPOINT_PATH,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{GDRIVE_PATH}/logs\",\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "    )\n",
        "\n",
        "    print(\"Creating trainer...\")\n",
        "    loss_fn = WeightedBCELoss(class_weights)\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_ds[\"train\"],\n",
        "        eval_dataset=tokenized_ds[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        loss_function=loss_fn\n",
        "    )\n",
        "\n",
        "    # Train and save\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    trainer.save_model(MODEL_PATH)\n",
        "    tokenizer.save_pretrained(MODEL_PATH)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EVALUATION ONLY CODE ---\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "# Disable W&B explicitly\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Configuration (same as training)\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/allergen_detection\"\n",
        "MODEL_PATH = f\"{GDRIVE_PATH}/model\"\n",
        "\n",
        "# 3. Load original allergens\n",
        "with open(f\"{GDRIVE_PATH}/evaluation_data/allergens.txt\", \"r\") as f:\n",
        "    original_allergens = f.read().splitlines()\n",
        "\n",
        "# 4. Class Consolidation Mapping (must match training)\n",
        "CONSOLIDATION_MAP = {\n",
        "    \"Tree Nuts\": [\n",
        "        \"Almonds\", \"Cashews\", \"Pistachios\", \"Walnuts\", \"Pecans\",\n",
        "        \"Hazelnuts\", \"Macadamia\", \"Brazil Nuts\", \"Pine Nuts\", \"Chestnuts\"\n",
        "    ],\n",
        "    \"Shellfish and Molluscs\": [\"Shrimp\", \"Crab\", \"Squid\", \"Cuttlefish\", \"Molluscs\", \"Lobster\"],\n",
        "    \"Fish\": [\n",
        "        \"Anchovy\", \"Basa\", \"Bass\", \"Cod\", \"Hake\", \"Perch\",\n",
        "        \"Pollock\", \"Salmon\", \"Sole\", \"Tuna\"\n",
        "    ],\n",
        "    \"Grains\": [\n",
        "        \"Barley\", \"Rye\", \"Oats\", \"Spelt\", \"Triticale\", \"Durum\",\n",
        "        \"Emmer\", \"Farro\", \"Semolina\", \"Couscous\", \"Bulgur\", \"Malt\"\n",
        "    ],\n",
        "    \"Legumes\": [\n",
        "        \"Lentils\", \"Chickpeas\", \"Peas\", \"Lima Beans\", \"Kidney Beans\",\n",
        "        \"Black Beans\", \"Pinto Beans\", \"Navy Beans\", \"Soybeans\"\n",
        "    ],\n",
        "    \"Dairy Derivatives\": [\"Casein\", \"Whey\", \"Lactose\", \"Caseinate\", \"Paneer\"],\n",
        "    \"Seeds\": [\n",
        "        \"Sunflower Seeds\", \"Pumpkin Seeds\", \"Chia Seeds\",\n",
        "        \"Cottonseed\", \"Rapeseed\", \"Canola\"\n",
        "    ],\n",
        "    \"Fruits\": [\n",
        "        \"Kiwi\", \"Mango\", \"Banana\", \"Avocado\", \"Strawberry\", \"Citrus\",\n",
        "        \"Apple\", \"Peach\", \"Cherry\", \"Grape\", \"Melon\", \"Tomato\",\n",
        "        \"Papaya\", \"Pineapple\", \"Passion Fruit\"\n",
        "    ],\n",
        "    \"Vegetables\": [\n",
        "        \"Celery\", \"Carrot\", \"Bell Pepper\", \"Chili Pepper\", \"Garlic\",\n",
        "        \"Onion\", \"Broccoli\", \"Cabbage\", \"Cauliflower\", \"Spinach\",\n",
        "        \"Beetroot\", \"Zucchini\"\n",
        "    ],\n",
        "    \"Spices\": [\n",
        "        \"Cumin\", \"Coriander\", \"Fennel\", \"Paprika\", \"Turmeric\",\n",
        "        \"Cinnamon\", \"Cloves\", \"Nutmeg\", \"Mustard\", \"Pepper\",\n",
        "        \"Vanilla\", \"Anise\", \"Cardamom\", \"Wasabi\", \"Horseradish\"\n",
        "    ],\n",
        "    \"Food Additives\": [\n",
        "        \"Sulfites\", \"Nitrates\", \"MSG\", \"Tartrazine\", \"Carmine\",\n",
        "        \"Annatto\", \"Aspartame\", \"Saccharin\", \"Sorbic Acid\",\n",
        "        \"Propylene Glycol\", \"Cochineal\", \"Shellac\"\n",
        "    ],\n",
        "    \"Alternative Grains\": [\n",
        "        \"Buckwheat\", \"Quinoa\", \"Amaranth\", \"Sorghum\", \"Millet\"\n",
        "    ],\n",
        "    \"Animal Proteins\": [\n",
        "        \"Beef\", \"Pork\", \"Chicken\", \"Turkey\", \"Lamb\",\n",
        "        \"Venison\", \"Goat\", \"Duck\", \"Gelatin\"\n",
        "    ],\n",
        "    \"Plant Proteins\": [\"Soy Milk\", \"Coconut Milk\", \"Seitan\", \"Tofu\"]\n",
        "}\n",
        "\n",
        "KEEP_SEPARATE = [\n",
        "    \"Eggs\", \"Peanuts\", \"Soy\", \"Wheat\", \"Sesame\", \"Gluten\", \"Sulphur Dioxide\"\n",
        "]\n",
        "\n",
        "# 5. Recreate consolidated classes\n",
        "CONSOLIDATED_ALLERGENS = [\n",
        "    cls for cls in original_allergens\n",
        "    if not any(cls in v for v in CONSOLIDATION_MAP.values())\n",
        "] + list(CONSOLIDATION_MAP.keys())\n",
        "\n",
        "# 6. Helper functions (same as training)\n",
        "def generate_labels(text):\n",
        "    \"\"\"Generate binary labels based on presence of original allergens.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    labels = np.zeros(len(original_allergens), dtype=np.float32)\n",
        "    for i, allergen in enumerate(original_allergens):\n",
        "        if allergen.lower() in text_lower:\n",
        "            labels[i] = 1.0\n",
        "    return labels\n",
        "\n",
        "def consolidate_labels(labels, original_classes=original_allergens, consolidation_map=CONSOLIDATION_MAP):\n",
        "    \"\"\"Convert original labels to consolidated labels.\"\"\"\n",
        "    # Get the list of consolidated classes\n",
        "    consolidated_classes = [\n",
        "        cls for cls in original_classes\n",
        "        if not any(cls in v for v in consolidation_map.values())  # Fix: .values() instead of .items()\n",
        "    ] + list(consolidation_map.keys())\n",
        "\n",
        "    consolidated = np.zeros((len(labels), len(consolidated_classes)), dtype=np.float32)\n",
        "\n",
        "    # Create mapping from original to consolidated indices\n",
        "    class_mapping = {}\n",
        "    for i, cls in enumerate(original_classes):\n",
        "        # Check if this class is part of a consolidated group\n",
        "        found = False\n",
        "        for group_name, members in consolidation_map.items():\n",
        "            if cls in members:\n",
        "                group_idx = consolidated_classes.index(group_name)\n",
        "                class_mapping[i] = group_idx\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            # Check if class should be kept separate\n",
        "            if cls in consolidated_classes:\n",
        "                class_mapping[i] = consolidated_classes.index(cls)\n",
        "\n",
        "    # Aggregate labels\n",
        "    for orig_idx, cons_idx in class_mapping.items():\n",
        "        consolidated[:, cons_idx] = np.logical_or(\n",
        "            consolidated[:, cons_idx],\n",
        "            labels[:, orig_idx]\n",
        "        )\n",
        "\n",
        "    return consolidated\n",
        "\n",
        "# 8. Tokenization (same as training)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "def process_dataset(dataset):\n",
        "    \"\"\"Process dataset with consolidated labels\"\"\"\n",
        "    original_labels = [generate_labels(ex[\"text\"]) for ex in dataset]\n",
        "    consolidated = consolidate_labels(np.array(original_labels))\n",
        "    return Dataset.from_dict({\n",
        "        \"text\": [ex[\"text\"] for ex in dataset],\n",
        "        \"labels\": consolidated\n",
        "    }).filter(lambda x: x[\"text\"] != \"\").with_format(\"torch\")\n",
        "\n",
        "def load_test_data():\n",
        "    raw_data = load_dataset(\"raphael0202/ingredient-detection-layout-dataset\")[\"train\"]\n",
        "    processed = process_dataset(raw_data).train_test_split(test_size=0.2, seed=42)[\"test\"]\n",
        "    return processed\n",
        "\n",
        "# 9. Load model and prepare test set\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "test_dataset = load_test_data().map(tokenize, batched=True)\n",
        "\n",
        "# 10. Create evaluator\n",
        "evaluator = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./tmp\",\n",
        "        per_device_eval_batch_size=16,\n",
        "        disable_tqdm=False\n",
        "    )\n",
        ")\n",
        "\n",
        "# 11. Generate predictions\n",
        "predictions = evaluator.predict(test_dataset)\n",
        "probs = torch.sigmoid(torch.tensor(predictions.predictions))\n",
        "y_pred = (probs > 0.5).int().numpy()\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# 12. Calculate metrics\n",
        "print(\"\\\\nDetailed Performance:\\\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=CONSOLIDATED_ALLERGENS, zero_division=0))\n",
        "\n",
        "print(\"\\\\nSummary Metrics:\")\n",
        "print(f\"Micro F1: {precision_recall_fscore_support(y_true, y_pred, average='micro')[2]:.4f}\")\n",
        "print(f\"Exact Match Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "try:\n",
        "    print(f\"ROC-AUC: {roc_auc_score(y_true, probs, average='micro'):.4f}\")\n",
        "except ValueError:\n",
        "    print(\"ROC-AUC unavailable (missing classes in predictions)\")"
      ],
      "metadata": {
        "id": "pvVhSw6Zy2Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Configuration\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/allergen_detection\"\n",
        "MODEL_PATH = f\"{GDRIVE_PATH}/model\"\n",
        "\n",
        "# Define consolidation map\n",
        "CONSOLIDATION_MAP = {\n",
        "    \"Milk\": [\"casein\", \"whey\", \"lactose\"],\n",
        "    \"Tree Nuts\": [\"almond\", \"hazelnut\", \"walnut\", \"pecan\", \"cashew\", \"pistachio\", \"macadamia\"],\n",
        "    \"Fish\": [\"cod\", \"salmon\", \"tuna\", \"tilapia\", \"haddock\", \"pollock\"],\n",
        "    \"Shellfish\": [\"shrimp\", \"crab\", \"lobster\", \"clam\", \"mussel\", \"oyster\"],\n",
        "    \"Wheat\": [\"spelt\", \"semolina\", \"durum\", \"farro\"]\n",
        "}\n",
        "\n",
        "# Install required packages\n",
        "def setup_dependencies():\n",
        "    try:\n",
        "        import cv2\n",
        "    except ImportError:\n",
        "        print(\"Installing OpenCV...\")\n",
        "        !pip install opencv-python-headless\n",
        "\n",
        "    try:\n",
        "        import pytesseract\n",
        "    except ImportError:\n",
        "        print(\"Installing pytesseract...\")\n",
        "        !pip install pytesseract\n",
        "\n",
        "    # Install Tesseract OCR in Colab\n",
        "    !apt-get install -qq tesseract-ocr\n",
        "    !pip install pytesseract\n",
        "\n",
        "    # Set tesseract path\n",
        "    pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n",
        "    print(\"Dependencies installed successfully\")\n",
        "\n",
        "# Model loading\n",
        "def load_model_and_resources():\n",
        "    print(\"Loading model and resources...\")\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"Model not found at {MODEL_PATH}\")\n",
        "\n",
        "    try:\n",
        "        config = BertForSequenceClassification.config_class.from_pretrained(MODEL_PATH)\n",
        "        print(f\"Model configuration loaded: {config.num_labels} output classes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model config: {e}\")\n",
        "\n",
        "    try:\n",
        "        model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "        print(\"Model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "        print(\"Tokenizer loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading tokenizer: {e}\")\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    allergens_file = f\"{GDRIVE_PATH}/evaluation_data/allergens.txt\"\n",
        "    try:\n",
        "        with open(allergens_file, \"r\") as f:\n",
        "            original_allergens = f.read().splitlines()\n",
        "        print(f\"Loaded {len(original_allergens)} allergen classes\")\n",
        "    except FileNotFoundError:\n",
        "        original_allergens = [\n",
        "            \"Milk\", \"Eggs\", \"Fish\", \"Shellfish\", \"Tree Nuts\",\n",
        "            \"Peanuts\", \"Wheat\", \"Soybeans\", \"Sesame\", \"Mustard\",\n",
        "            \"Celery\", \"Lupin\", \"Molluscs\", \"Sulphites\"\n",
        "        ]\n",
        "        print(f\"Using fallback allergen list with {len(original_allergens)} classes\")\n",
        "\n",
        "    consolidated_allergens = [\n",
        "        cls for cls in original_allergens\n",
        "        if not any(cls in v for v in CONSOLIDATION_MAP.values())\n",
        "    ] + list(CONSOLIDATION_MAP.keys())\n",
        "\n",
        "    print(f\"Final consolidated allergen classes: {len(consolidated_allergens)}\")\n",
        "\n",
        "    return model, tokenizer, consolidated_allergens, device\n",
        "\n",
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    typo_corrections = {\n",
        "        \"cotains\": \"contains\",\n",
        "        \"containts\": \"contains\",\n",
        "        \"containss\": \"contains\",\n",
        "        \"allergy\": \"allergen\",\n",
        "        \"glutten\": \"gluten\",\n",
        "        \"peenuts\": \"peanuts\"\n",
        "    }\n",
        "\n",
        "    for typo, correction in typo_corrections.items():\n",
        "        text = text.replace(typo, correction)\n",
        "\n",
        "    if not any(phrase in text for phrase in [\"contains\", \"may contain\", \"ingredients:\"]):\n",
        "        if len(text.strip().split()) < 3:\n",
        "            text = f\"contains {text}\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Prediction function\n",
        "def predict_allergens(text, model, tokenizer, consolidated_classes, device, threshold=0.3, debug=False):\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Original text: {text}\")\n",
        "        print(f\"Processed text: {processed_text}\")\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        processed_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Tokenized input: {tokenizer.decode(inputs['input_ids'][0])}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.sigmoid(outputs.logits).cpu().numpy().flatten()\n",
        "\n",
        "    if debug:\n",
        "        print(\"\\nRaw prediction probabilities:\")\n",
        "        for i, (prob, allergen) in enumerate(zip(probs, consolidated_classes)):\n",
        "            print(f\"{allergen}: {prob:.4f}\")\n",
        "\n",
        "    predictions = []\n",
        "    for i, (prob, allergen) in enumerate(zip(probs, consolidated_classes)):\n",
        "        if prob >= threshold:\n",
        "            predictions.append((allergen, f\"{prob:.2f}\"))\n",
        "\n",
        "    predictions.sort(key=lambda x: float(x[1]), reverse=True)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# OCR processing\n",
        "def extract_text_from_image(image_path, debug=False):\n",
        "    try:\n",
        "        if isinstance(image_path, str):\n",
        "            image = cv2.imread(image_path)\n",
        "        elif isinstance(image_path, np.ndarray):\n",
        "            image = image_path\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image input type\")\n",
        "\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image from {image_path}\")\n",
        "\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "        text = pytesseract.image_to_string(thresh)\n",
        "\n",
        "        if debug:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.subplot(121)\n",
        "            plt.title(\"Original Image\")\n",
        "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            plt.subplot(122)\n",
        "            plt.title(\"Preprocessed for OCR\")\n",
        "            plt.imshow(thresh, cmap='gray')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            print(f\"Extracted Text:\\n{text}\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OCR: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "# Detection pipeline\n",
        "def detect_allergens_from_image(image_path, model, tokenizer, consolidated_classes, device, threshold=0.3, debug=False):\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(\"Extracting text from image...\")\n",
        "    extracted_text = extract_text_from_image(image_path, debug=debug)\n",
        "\n",
        "    if not extracted_text.strip():\n",
        "        print(\"No text extracted from image.\")\n",
        "        return []\n",
        "\n",
        "    ocr_time = time.time() - start_time\n",
        "\n",
        "    lines = extracted_text.split('\\n')\n",
        "    ingredient_sections = []\n",
        "    ingredients_started = False\n",
        "    current_section = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if any(keyword in line.lower() for keyword in ['ingredients', 'contains', 'allergens', 'may contain']):\n",
        "            ingredients_started = True\n",
        "            current_section = line\n",
        "        elif ingredients_started:\n",
        "            if len(line) > 3:\n",
        "                current_section += \" \" + line\n",
        "\n",
        "        if current_section and len(current_section) > 20:\n",
        "            ingredient_sections.append(current_section)\n",
        "            current_section = \"\"\n",
        "            ingredients_started = False\n",
        "\n",
        "    if current_section:\n",
        "        ingredient_sections.append(current_section)\n",
        "\n",
        "    if not ingredient_sections:\n",
        "        ingredient_sections = [extracted_text]\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    for section in ingredient_sections:\n",
        "        if debug:\n",
        "            print(f\"\\nAnalyzing section: {section}\")\n",
        "\n",
        "        section_predictions = predict_allergens(section, model, tokenizer, consolidated_classes, device, threshold, debug)\n",
        "\n",
        "        for pred in section_predictions:\n",
        "            if pred not in all_predictions:\n",
        "                all_predictions.append(pred)\n",
        "\n",
        "    inference_time = time.time() - start_time - ocr_time\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"OCR time: {ocr_time:.2f} s\")\n",
        "    print(f\"Inference time: {inference_time:.2f} s\")\n",
        "    print(f\"Total processing time: {total_time:.2f} s\")\n",
        "\n",
        "    return all_predictions, extracted_text\n",
        "\n",
        "# Interactive interface with upload support\n",
        "def interactive_testing_with_images(model, tokenizer, consolidated_classes, device, threshold=0.3):\n",
        "    print(\"\\n=== Interactive Allergen Detection with Image Support ===\")\n",
        "    print(f\"Using detection threshold: {threshold}\")\n",
        "    print(\"Commands:\")\n",
        "    print(\"  'text' - Enter text mode for text-based detection\")\n",
        "    print(\"  'image' - Enter image mode for image-based detection\")\n",
        "    print(\"  'debug' - Toggle debug mode\")\n",
        "    print(\"  'threshold X' - Set threshold to X (e.g., 'threshold 0.4')\")\n",
        "    print(\"  'exit' - Quit the program\")\n",
        "    print(\"\\nIn IMAGE MODE: Press Enter to upload an image directly\")\n",
        "\n",
        "    debug_mode = False\n",
        "    mode = \"text\"\n",
        "\n",
        "    while True:\n",
        "        if mode == \"text\":\n",
        "            prompt = \"\\n[TEXT MODE] Enter text (or command): \"\n",
        "        else:\n",
        "            prompt = \"\\n[IMAGE MODE] Enter image path/URL, press Enter to upload, or command: \"\n",
        "\n",
        "        user_input = input(prompt)\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        elif user_input.lower() == 'debug':\n",
        "            debug_mode = not debug_mode\n",
        "            print(f\"Debug mode: {'ON' if debug_mode else 'OFF'}\")\n",
        "            continue\n",
        "\n",
        "        elif user_input.lower() == 'text':\n",
        "            mode = \"text\"\n",
        "            print(\"Switched to TEXT MODE\")\n",
        "            continue\n",
        "\n",
        "        elif user_input.lower() == 'image':\n",
        "            mode = \"image\"\n",
        "            print(\"Switched to IMAGE MODE\")\n",
        "            continue\n",
        "\n",
        "        elif user_input.lower().startswith('threshold'):\n",
        "            try:\n",
        "                parts = user_input.split()\n",
        "                if len(parts) != 2:\n",
        "                    print(\"Usage: threshold X (e.g., threshold 0.4)\")\n",
        "                    continue\n",
        "\n",
        "                new_threshold = float(parts[1])\n",
        "                if 0 <= new_threshold <= 1:\n",
        "                    threshold = new_threshold\n",
        "                    print(f\"Threshold updated to {threshold}\")\n",
        "                else:\n",
        "                    print(\"Threshold must be between 0.0 and 1.0\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid threshold value. Please enter a number between 0.0 and 1.0\")\n",
        "            continue\n",
        "\n",
        "        if mode == \"text\":\n",
        "            start_time = time.time()\n",
        "            predictions = predict_allergens(user_input, model, tokenizer, consolidated_classes, device, threshold, debug=debug_mode)\n",
        "            inference_time = (time.time() - start_time) * 1000\n",
        "\n",
        "            print(f\"\\nInference time: {inference_time:.2f} ms\")\n",
        "\n",
        "            if predictions:\n",
        "                print(\"Detected Allergens:\")\n",
        "                for allergen, confidence in predictions:\n",
        "                    print(f\"- {allergen} (Confidence: {confidence})\")\n",
        "            else:\n",
        "                print(\"No allergens detected above threshold.\")\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                if not user_input.strip():\n",
        "                    print(\"\\nPlease upload an image file...\")\n",
        "                    uploaded = files.upload()\n",
        "                    if not uploaded:\n",
        "                        print(\"No files uploaded. Please try again.\")\n",
        "                        continue\n",
        "                    image_path = list(uploaded.keys())[0]\n",
        "                    image = cv2.imread(image_path)\n",
        "                else:\n",
        "                    image_path = user_input.strip()\n",
        "                    if image_path.startswith(('http://', 'https://')):\n",
        "                        print(f\"Downloading image from {image_path}...\")\n",
        "                        response = requests.get(image_path)\n",
        "                        image = np.array(Image.open(io.BytesIO(response.content)))\n",
        "                        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "                    else:\n",
        "                        if not os.path.exists(image_path):\n",
        "                            print(f\"File not found: {image_path}\")\n",
        "                            continue\n",
        "                        image = cv2.imread(image_path)\n",
        "\n",
        "                if image is None:\n",
        "                    raise ValueError(\"Failed to load image\")\n",
        "\n",
        "                predictions, extracted_text = detect_allergens_from_image(\n",
        "                    image, model, tokenizer, consolidated_classes, device, threshold, debug=debug_mode\n",
        "                )\n",
        "\n",
        "                print(\"\\nExtracted Text:\")\n",
        "                print(\"--------------\")\n",
        "                print(extracted_text)\n",
        "                print(\"--------------\")\n",
        "\n",
        "                if predictions:\n",
        "                    print(\"\\nDetected Allergens:\")\n",
        "                    for allergen, confidence in predictions:\n",
        "                        print(f\"- {allergen} (Confidence: {confidence})\")\n",
        "                else:\n",
        "                    print(\"\\nNo allergens detected above threshold.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    print(\"Allergen Detection System with Image Support\")\n",
        "    print(\"===========================================\")\n",
        "\n",
        "    try:\n",
        "        setup_dependencies()\n",
        "        model, tokenizer, consolidated_allergens, device = load_model_and_resources()\n",
        "        interactive_testing_with_images(model, tokenizer, consolidated_allergens, device, threshold=0.3)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "thNn9-AUzBPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "id": "BtZwlsdzkHh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Allergen Detection System with Image Upload and Advanced Scanning\n",
        "import re\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "from rapidfuzz import fuzz, process\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "\n",
        "# Add standard file upload support\n",
        "import base64\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import os\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class EnhancedAllergenDetector:\n",
        "    def __init__(self, model, tokenizer, allergen_classes):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.allergen_classes = allergen_classes\n",
        "        self._init_settings()\n",
        "\n",
        "    def _init_settings(self):\n",
        "        \"\"\"Initialize detection parameters and dictionaries\"\"\"\n",
        "        # Multiple OCR configurations to try for better text extraction\n",
        "        self.ocr_configs = [\n",
        "            r'--oem 3 --psm 6 -c preserve_interword_spaces=1',  # Standard paragraph mode\n",
        "            r'--oem 3 --psm 4 -c preserve_interword_spaces=1',  # Multiple column mode\n",
        "            r'--oem 3 --psm 11 --dpi 300 -c textord_min_linesize=1.5',  # Raw line mode with higher DPI\n",
        "            r'--oem 3 --psm 1 -c preserve_interword_spaces=1 -c textord_min_linesize=1.5',  # Auto page segmentation\n",
        "            r'--oem 3 --psm 3 -l eng -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,.:;()-_ '  # Limited char set\n",
        "        ]\n",
        "\n",
        "        # Increased confidence thresholds for allergen detection\n",
        "        self.thresholds = {\n",
        "            'Milk': 0.25, 'Eggs': 0.3, 'Peanuts': 0.2,\n",
        "            'Tree Nuts': 0.3, 'Soy': 0.3, 'Wheat': 0.3,\n",
        "            'Fish': 0.4, 'Shellfish': 0.4, 'Sesame': 0.25,\n",
        "            'Gluten': 0.35, 'Sulphur Dioxide': 0.3,\n",
        "            'DEFAULT': 0.35\n",
        "        }\n",
        "\n",
        "        # Expanded OCR corrections with more common errors and variants\n",
        "        self.ocr_corrections = {\n",
        "            # Original corrections\n",
        "            r'\\bmaelk\\b': 'milk', r'\\bweat\\b': 'wheat',\n",
        "            r'\\bsoya\\b': 'soy', r'\\bnutt?a\\b': 'nut',\n",
        "            r'\\bcaseinates?\\b': 'casein', r'\\bgluten\\b': 'gluten',\n",
        "            r'\\bmilx\\b': 'milk', r'\\bmllk\\b': 'milk',\n",
        "            r'\\beg+s\\b': 'eggs', r'\\bwhert\\b': 'wheat',\n",
        "            r'\\bl(\\W)?actose\\b': 'lactose', r'\\bsheIIfish\\b': 'shellfish',\n",
        "            r'\\bshelI\\W?fish\\b': 'shellfish', r'\\btree\\W?nut[sz]\\b': 'tree nuts',\n",
        "            r'\\bpenau?t[sz]\\b': 'peanuts', r'\\bsezame\\b': 'sesame',\n",
        "            r'\\bal(\\W)?mond[sz]\\b': 'almonds', r'\\bcashew[sz]\\b': 'cashews',\n",
        "            r'\\bwaInut[sz]\\b': 'walnuts', r'\\bhazelnut[sz]\\b': 'hazelnuts',\n",
        "            r'\\bwhey\\b': 'whey', r'\\blacto[sz]e\\b': 'lactose',\n",
        "\n",
        "            # New corrections for common OCR errors\n",
        "            r'\\b[a-z]{1,2}\\b': '',  # Remove 1-2 letter garbage tokens\n",
        "            r'[^\\w\\s,.:-]': '',  # Remove unusual symbols\n",
        "            r'\\s{2,}': ' ',  # Normalize spacing\n",
        "            r'(\\w)\\.(\\w)': r'\\1\\2',  # Fix period in the middle of words\n",
        "            r'(\\d),(\\d)': r'\\1.\\2',  # Fix commas misrecognized as decimal points\n",
        "            r'rnilk': 'milk',  # Common 'm' misread as 'rn'\n",
        "            r'rnay': 'may',    # Common 'm' misread as 'rn'\n",
        "            r'conta[il]ns': 'contains',  # common errors in \"contains\"\n",
        "            r'aIIerg[ce]n': 'allergen',  # Common errors in \"allergen\"\n",
        "            r'\\bmay\\s+c': 'may contain',  # Fix truncation of \"may contain\"\n",
        "            r'trac[ce][sz]': 'traces',  # Fix errors in \"traces\"\n",
        "            r'([a-z])I([a-z])': r'\\1l\\2',  # Fix capital I misread as lowercase L\n",
        "            r'([a-z])1([a-z])': r'\\1l\\2',  # Fix 1 misread as lowercase L\n",
        "            r'([A-Z])l([a-z])': r'\\1I\\2',  # Fix lowercase l misread as capital I\n",
        "            r'([A-Z])1([a-z])': r'\\1I\\2',  # Fix 1 misread as capital I\n",
        "            r'(\\s)l(\\s)': r'\\1I\\2',  # Fix standalone lowercase l as I\n",
        "            r'(\\s)1(\\s)': r'\\1I\\2',  # Fix standalone 1 as I\n",
        "            r'\\bO\\b': '0',  # Fix letter O as number 0\n",
        "            r'\\bo\\b': '0',  # Fix letter o as number 0\n",
        "            r'vv': 'w',     # Fix double v as w\n",
        "            r'VV': 'W',     # Fix double V as W\n",
        "            r'rr\\b': 'm',   # Fix some instances of rr as m at word end\n",
        "            r'\\brn': 'm',   # Fix rn as m at word start\n",
        "        }\n",
        "\n",
        "        # Expand context phrases that indicate allergen presence\n",
        "        self.context_boost = {\n",
        "            'contains': 1.3, 'may contain': 1.2,\n",
        "            'traces of': 1.1, 'facility': 1.05,\n",
        "            'allergen': 1.25, 'warning': 1.2,\n",
        "            'produced in': 1.05, 'processed on': 1.05,\n",
        "            'ingredients': 1.1, 'manufactured': 1.05,\n",
        "            'allegen information': 1.3, 'made with': 1.15,\n",
        "            'contains traces': 1.2, 'allergy advice': 1.3,\n",
        "            'allergens': 1.3, 'free from': 0.8,  # Lower confidence for \"free from\"\n",
        "        }\n",
        "\n",
        "        # Common allergen synonyms and related words\n",
        "        self.allergen_synonyms = {\n",
        "            'Milk': ['dairy', 'lactose', 'whey', 'casein', 'cream', 'butter', 'cheese', 'milk powder', 'yogurt', 'yoghurt'],\n",
        "            'Eggs': ['egg white', 'egg yolk', 'albumin', 'lysozyme', 'ovalbumin', 'egg powder', 'dried egg'],\n",
        "            'Peanuts': ['arachis', 'goober', 'groundnut', 'earthnut', 'monkey nut', 'peanut butter'],\n",
        "            'Tree Nuts': ['almond', 'walnut', 'cashew', 'hazelnut', 'pecan', 'pistachio', 'macadamia', 'brazil nut', 'pine nut'],\n",
        "            'Soy': ['soya', 'soybeans', 'edamame', 'tofu', 'tempeh', 'miso', 'soya lecithin', 'soy protein'],\n",
        "            'Wheat': ['flour', 'semolina', 'durum', 'spelt', 'kamut', 'couscous', 'pasta', 'bread', 'bulgur'],\n",
        "            'Fish': ['cod', 'salmon', 'tuna', 'haddock', 'pollock', 'tilapia', 'bass', 'sardine', 'anchovy', 'seafood'],\n",
        "            'Shellfish': ['shrimp', 'crab', 'lobster', 'prawn', 'crayfish', 'oyster', 'mussel', 'scallop', 'clam', 'mollusk'],\n",
        "            'Sesame': ['sesame seed', 'tahini', 'sesamol', 'goma', 'sesame oil', 'sesame flour'],\n",
        "            'Gluten': ['wheat', 'rye', 'barley', 'oats', 'spelt', 'triticale', 'malt', 'seitan']\n",
        "        }\n",
        "\n",
        "        # Common allergen warning phrases\n",
        "        self.allergen_warning_phrases = [\n",
        "            \"allergy advice\", \"allergen information\", \"contains\", \"may contain\",\n",
        "            \"produced in a facility\", \"manufactured on shared equipment\",\n",
        "            \"traces of\", \"warning\", \"allergen statement\", \"for allergens\",\n",
        "            \"allergen warning\", \"free from\", \"not suitable for\", \"avoid if\"\n",
        "        ]\n",
        "\n",
        "    def preprocess_image(self, image, preprocessing_method='adaptive'):\n",
        "        \"\"\"Enhanced image preprocessing with multiple options\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if preprocessing_method == 'adaptive':\n",
        "            # Adaptive thresholding (good for varying lighting conditions)\n",
        "            processed = cv2.adaptiveThreshold(\n",
        "                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                cv2.THRESH_BINARY, 11, 2\n",
        "            )\n",
        "            # Add denoising step\n",
        "            return cv2.fastNlMeansDenoising(processed, None, 10, 7, 21)\n",
        "\n",
        "        elif preprocessing_method == 'otsu':\n",
        "            # Otsu's thresholding (good for bimodal images)\n",
        "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "            _, processed = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            return processed\n",
        "\n",
        "        elif preprocessing_method == 'contrast':\n",
        "            # Contrast enhancement\n",
        "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "            return clahe.apply(gray)\n",
        "\n",
        "        elif preprocessing_method == 'bilateral':\n",
        "            # Bilateral filtering (preserves edges while removing noise)\n",
        "            filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n",
        "            # Apply adaptive threshold\n",
        "            thresh = cv2.adaptiveThreshold(\n",
        "                filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                cv2.THRESH_BINARY, 11, 2\n",
        "            )\n",
        "            return thresh\n",
        "\n",
        "        elif preprocessing_method == 'sharpen':\n",
        "            # Image sharpening\n",
        "            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "            sharpened = cv2.filter2D(gray, -1, kernel)\n",
        "            # Convert to binary\n",
        "            _, thresh = cv2.threshold(sharpened, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            return thresh\n",
        "\n",
        "        else:\n",
        "            # Default to adaptive\n",
        "            processed = cv2.adaptiveThreshold(\n",
        "                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                cv2.THRESH_BINARY, 11, 2\n",
        "            )\n",
        "            return cv2.fastNlMeansDenoising(processed, None, 10, 7, 21)\n",
        "\n",
        "    def locate_allergen_section(self, image):\n",
        "        \"\"\"Attempt to locate the allergen section of the packaging\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Define scale factor for pyramid\n",
        "        scale_factor = 0.75\n",
        "        min_size = (30, 30)\n",
        "\n",
        "        # Create a pyramid of scaled images to handle different sized text\n",
        "        current_img = gray.copy()\n",
        "        potential_regions = []\n",
        "\n",
        "        # Create a pyramid of scaled images\n",
        "        while current_img.shape[0] > min_size[0] and current_img.shape[1] > min_size[1]:\n",
        "            # Apply edge detection\n",
        "            edges = cv2.Canny(current_img, 50, 150)\n",
        "\n",
        "            # Find contours\n",
        "            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # Adjust contour coordinates to match original image scale\n",
        "            scale = gray.shape[0] / current_img.shape[0]\n",
        "\n",
        "            # Look for rectangular regions (typical for allergen sections)\n",
        "            for cnt in contours:\n",
        "                # Scale contour back to original size\n",
        "                cnt = (cnt * scale).astype(np.int32)\n",
        "\n",
        "                x, y, w, h = cv2.boundingRect(cnt)\n",
        "\n",
        "                # Filter for likely allergen regions based on size and aspect ratio\n",
        "                if 0.1 < w/h < 10 and 500 < w*h < (gray.shape[0] * gray.shape[1]) // 4:\n",
        "                    # Check for typical allergen warning words in this region\n",
        "                    roi = gray[y:y+h, x:x+w]\n",
        "                    text = pytesseract.image_to_string(roi, config='--psm 6')\n",
        "                    text = text.lower()\n",
        "\n",
        "                    # Check if this ROI contains allergen-related text\n",
        "                    if any(phrase in text for phrase in self.allergen_warning_phrases):\n",
        "                        potential_regions.append((x, y, w, h, 0.9))  # High confidence\n",
        "                    else:\n",
        "                        # Still add as a potential region but with lower confidence\n",
        "                        potential_regions.append((x, y, w, h, 0.5))\n",
        "\n",
        "            # Scale down image for next iteration\n",
        "            current_img = cv2.resize(current_img, (0, 0), fx=scale_factor, fy=scale_factor)\n",
        "\n",
        "        # Sort regions by confidence\n",
        "        potential_regions.sort(key=lambda x: x[4], reverse=True)\n",
        "\n",
        "        # Return top regions or all if few were found\n",
        "        return potential_regions[:3] if len(potential_regions) > 3 else potential_regions\n",
        "\n",
        "    def extract_text(self, image):\n",
        "        \"\"\"Perform OCR with multiple configurations and preprocessing methods, with confidence filtering\"\"\"\n",
        "        best_text = \"\"\n",
        "        best_word_count = 0\n",
        "        best_confidence = 0\n",
        "\n",
        "        # First try to locate allergen sections\n",
        "        allergen_regions = self.locate_allergen_section(image)\n",
        "\n",
        "        # If allergen regions found, prioritize these\n",
        "        if allergen_regions:\n",
        "            for x, y, w, h, conf in allergen_regions:\n",
        "                # Extract region\n",
        "                region = image[y:y+h, x:x+w]\n",
        "\n",
        "                # Apply our standard OCR procedure to this region\n",
        "                for method in ['adaptive', 'contrast', 'bilateral', 'sharpen', 'otsu']:\n",
        "                    processed = self.preprocess_image(region, method)\n",
        "\n",
        "                    for config in self.ocr_configs:\n",
        "                        try:\n",
        "                            # Use image_to_data to get confidence scores\n",
        "                            ocr_data = pytesseract.image_to_data(processed, config=config, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "                            # Filter out low confidence words (below 30%)\n",
        "                            valid_texts = []\n",
        "                            avg_confidence = 0\n",
        "                            valid_word_count = 0\n",
        "\n",
        "                            for i, text in enumerate(ocr_data['text']):\n",
        "                                if text and ocr_data['conf'][i] > 30:  # Only include words with confidence > 30%\n",
        "                                    valid_texts.append(text)\n",
        "                                    avg_confidence += ocr_data['conf'][i]\n",
        "                                    valid_word_count += 1\n",
        "\n",
        "                            if valid_word_count > 0:\n",
        "                                avg_confidence /= valid_word_count\n",
        "                                extracted_text = ' '.join(valid_texts)\n",
        "\n",
        "                                # If this is better than our previous best, update it\n",
        "                                if valid_word_count > best_word_count and avg_confidence > best_confidence:\n",
        "                                    best_text = extracted_text\n",
        "                                    best_word_count = valid_word_count\n",
        "                                    best_confidence = avg_confidence\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "        # If didn't get good results from allergen regions, try the full image\n",
        "        if best_word_count < 5:  # Arbitrary threshold for \"good enough\" results\n",
        "            # Try different preprocessing methods\n",
        "            processing_methods = ['adaptive', 'otsu', 'contrast', 'bilateral', 'sharpen']\n",
        "\n",
        "            for method in processing_methods:\n",
        "                processed = self.preprocess_image(image, method)\n",
        "\n",
        "                # Try different OCR configurations\n",
        "                for config in self.ocr_configs:\n",
        "                    try:\n",
        "                        ocr_data = pytesseract.image_to_data(processed, config=config, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "                        # Filter out low confidence words\n",
        "                        valid_texts = []\n",
        "                        avg_confidence = 0\n",
        "                        valid_word_count = 0\n",
        "\n",
        "                        for i, text in enumerate(ocr_data['text']):\n",
        "                            if text and ocr_data['conf'][i] > 30:  # Only include words with confidence > 30%\n",
        "                                valid_texts.append(text)\n",
        "                                avg_confidence += ocr_data['conf'][i]\n",
        "                                valid_word_count += 1\n",
        "\n",
        "                        if valid_word_count > 0:\n",
        "                            avg_confidence /= valid_word_count\n",
        "                            extracted_text = ' '.join(valid_texts)\n",
        "\n",
        "                            # If this is better than our previous best, update it\n",
        "                            if valid_word_count > best_word_count and avg_confidence > best_confidence:\n",
        "                                best_text = extracted_text\n",
        "                                best_word_count = valid_word_count\n",
        "                                best_confidence = avg_confidence\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "        # Apply OCR error correction to the best result\n",
        "        corrected_text = self.correct_ocr_errors(best_text)\n",
        "\n",
        "        # Additional post-processing step to clean up the text\n",
        "        corrected_text = self.post_process_text(corrected_text)\n",
        "\n",
        "        return corrected_text\n",
        "\n",
        "    def post_process_text(self, text):\n",
        "        \"\"\"Additional text cleaning steps after OCR correction\"\"\"\n",
        "        # Remove consecutive duplicated words\n",
        "        words = text.split()\n",
        "        unique_words = []\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            if i == 0 or word != words[i-1]:\n",
        "                unique_words.append(word)\n",
        "\n",
        "        cleaned_text = ' '.join(unique_words)\n",
        "\n",
        "        # Fix spacing around punctuation\n",
        "        cleaned_text = re.sub(r'\\s+([.,;:!?])', r'\\1', cleaned_text)\n",
        "\n",
        "        # Fix broken sentences (spaces followed by lowercase)\n",
        "        cleaned_text = re.sub(r'([.!?])\\s+([a-z])', lambda m: m.group(1) + ' ' + m.group(2).upper(), cleaned_text)\n",
        "\n",
        "        # Remove any remaining garbage characters or patterns\n",
        "        cleaned_text = re.sub(r'[^\\w\\s.,;:!?()\\[\\]\"\\'/-]', '', cleaned_text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "        # Auto-complete common allergen phrases if partially detected\n",
        "        for phrase in [\"may contain\", \"allergen information\", \"contains\"]:\n",
        "            parts = phrase.split()\n",
        "            if any(part in cleaned_text.lower() for part in parts):\n",
        "                # Check if have a partial match\n",
        "                for part in parts:\n",
        "                    if part in cleaned_text.lower() and phrase not in cleaned_text.lower():\n",
        "                        # Add full phrase hint\n",
        "                        hint = f\" [{phrase}?]\"\n",
        "                        if hint not in cleaned_text:\n",
        "                            cleaned_text += hint\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "    def correct_ocr_errors(self, text):\n",
        "        \"\"\"Fix common OCR mistakes with improved handling\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        text = text.lower()\n",
        "\n",
        "        # First pass - Fix structural issues\n",
        "        for pattern, correction in self.ocr_corrections.items():\n",
        "            text = re.sub(pattern, correction, text)\n",
        "\n",
        "        # Second pass - Deal with common OCR failures in allergen contexts\n",
        "        allergen_specific_fixes = {\n",
        "            r'c\\s*o\\s*n\\s*t\\s*a\\s*i\\s*n\\s*s': 'contains',\n",
        "            r'a\\s*l\\s*l\\s*e\\s*r\\s*g\\s*e\\s*n': 'allergen',\n",
        "            r'm\\s*a\\s*y\\s*c\\s*o\\s*n\\s*t\\s*a\\s*i\\s*n': 'may contain',\n",
        "            r't\\s*r\\s*a\\s*c\\s*e\\s*s': 'traces'\n",
        "        }\n",
        "\n",
        "        for pattern, correction in allergen_specific_fixes.items():\n",
        "            text = re.sub(pattern, correction, text)\n",
        "\n",
        "        # Try to reconstruct common allergen-related words\n",
        "        for allergen in self.allergen_classes + list(self.allergen_synonyms.keys()):\n",
        "            # Create pattern to match characters with spaces in between\n",
        "            pattern = r'\\b' + r'\\s*'.join(list(allergen.lower())) + r'\\b'\n",
        "            text = re.sub(pattern, allergen.lower(), text)\n",
        "\n",
        "        # Try to reconstruct common warning phrases\n",
        "        for phrase in self.allergen_warning_phrases:\n",
        "            pattern = r'\\b' + r'\\s*'.join(list(phrase.lower())) + r'\\b'\n",
        "            text = re.sub(pattern, phrase.lower(), text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def fuzzy_match(self, text):\n",
        "        \"\"\"Enhanced fuzzy matching with synonyms and contextual awareness\"\"\"\n",
        "        matches = {}\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "        # Check for direct allergen matches with improved sensitivity\n",
        "        for word in words:\n",
        "            if len(word) < 3: continue\n",
        "            match, score, _ = process.extractOne(\n",
        "                word, self.allergen_classes, scorer=fuzz.WRatio\n",
        "            )\n",
        "            if score > 80:  # Lower threshold for initial detection\n",
        "                matches[match] = max(matches.get(match, 0), score/100)\n",
        "\n",
        "        # Check for synonym matches with improved sensitivity\n",
        "        for allergen, synonyms in self.allergen_synonyms.items():\n",
        "            for word in words:\n",
        "                if len(word) < 3: continue\n",
        "                best_score = 0\n",
        "                for synonym in synonyms:\n",
        "                    score = fuzz.WRatio(word, synonym)\n",
        "                    best_score = max(best_score, score)\n",
        "\n",
        "                if best_score > 85:  # Lower threshold for synonyms\n",
        "                    matches[allergen] = max(matches.get(allergen, 0), best_score/100 * 0.9)\n",
        "\n",
        "        # Context-based sentence analysis\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.lower().strip()\n",
        "            if not sentence: continue\n",
        "\n",
        "            # Check if this is an allergen-related sentence\n",
        "            if any(phrase in sentence for phrase in self.allergen_warning_phrases):\n",
        "                # This sentence is likely about allergens - analyze closely\n",
        "                for allergen in self.allergen_classes:\n",
        "                    allergen_lower = allergen.lower()\n",
        "\n",
        "                    # Direct mention\n",
        "                    if allergen_lower in sentence:\n",
        "                        matches[allergen] = max(matches.get(allergen, 0), 0.95)\n",
        "\n",
        "                    # Fuzzy mention\n",
        "                    else:\n",
        "                        # Check for partial matches (\"milk products\" for \"milk\")\n",
        "                        if any(word.startswith(allergen_lower) or word.endswith(allergen_lower) for word in sentence.split()):\n",
        "                            matches[allergen] = max(matches.get(allergen, 0), 0.85)\n",
        "\n",
        "                # Check for synonym matches in allergen contexts with higher weight\n",
        "                for allergen, synonyms in self.allergen_synonyms.items():\n",
        "                    for synonym in synonyms:\n",
        "                        if synonym in sentence:\n",
        "                            matches[allergen] = max(matches.get(allergen, 0), 0.9)\n",
        "\n",
        "        return matches\n",
        "\n",
        "    def detect_allergen_sections(self, text):\n",
        "        \"\"\"Enhanced identification of sections of text that are likely allergen lists\"\"\"\n",
        "        allergen_sections = []\n",
        "\n",
        "        # Regular expressions for common allergen section headers with improved patterns\n",
        "        section_patterns = [\n",
        "            r\"allergen\\s+info(rmation)?:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"contains:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"may\\s+contain:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"ingredients:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"warning:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"allergy advice:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"free from:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"not suitable for:?(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"produced in a facility(.+?)(?=\\n\\s*\\n|\\Z)\",\n",
        "            r\"traces of:?(.+?)(?=\\n\\s*\\n|\\Z)\"\n",
        "        ]\n",
        "\n",
        "        for pattern in section_patterns:\n",
        "            matches = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if matches and matches.group(1):\n",
        "                allergen_sections.append(matches.group(1).strip())\n",
        "\n",
        "        # If no sections found with the patterns, try to identify allergen-dense parts\n",
        "        if not allergen_sections:\n",
        "            # Split text into sentences or meaningful chunks\n",
        "            chunks = re.split(r'[.!?\\n]+', text)\n",
        "\n",
        "            for chunk in chunks:\n",
        "                chunk = chunk.strip()\n",
        "                if not chunk: continue\n",
        "\n",
        "                # Count allergen-related terms in this chunk\n",
        "                count = 0\n",
        "                for phrase in self.allergen_warning_phrases:\n",
        "                    if phrase in chunk.lower():\n",
        "                        count += 2  # Warning phrases have higher weight\n",
        "\n",
        "                for allergen in self.allergen_classes:\n",
        "                    if allergen.lower() in chunk.lower():\n",
        "                        count += 1\n",
        "\n",
        "                for allergen, synonyms in self.allergen_synonyms.items():\n",
        "                    for synonym in synonyms:\n",
        "                        if synonym in chunk.lower():\n",
        "                            count += 1\n",
        "\n",
        "                # If this chunk has multiple allergen terms, it's likely an allergen section\n",
        "                if count >= 2:\n",
        "                    allergen_sections.append(chunk)\n",
        "\n",
        "        return allergen_sections\n",
        "\n",
        "    def apply_context_rules(self, text, predictions):\n",
        "        \"\"\"Enhanced context rules with improved section detection\"\"\"\n",
        "        if not text:\n",
        "            return predictions\n",
        "\n",
        "        text = text.lower()\n",
        "\n",
        "        # Apply general context boosting\n",
        "        for phrase, boost in self.context_boost.items():\n",
        "            if phrase in text:\n",
        "                for allergen in predictions:\n",
        "                    predictions[allergen] = min(1.0, predictions[allergen] * boost)\n",
        "\n",
        "        # Apply targeted boosting for allergen sections\n",
        "        allergen_sections = self.detect_allergen_sections(text)\n",
        "        if allergen_sections:\n",
        "            combined_sections = \" \".join(allergen_sections)\n",
        "            for allergen in predictions:\n",
        "                allergen_lower = allergen.lower()\n",
        "                # Higher boost if allergen is mentioned in a dedicated allergen section\n",
        "                if allergen_lower in combined_sections:\n",
        "                    predictions[allergen] = min(1.0, predictions[allergen] * 1.5)\n",
        "\n",
        "                # Check for synonyms in allergen sections\n",
        "                if allergen in self.allergen_synonyms:\n",
        "                    for synonym in self.allergen_synonyms[allergen]:\n",
        "                        if synonym in combined_sections:\n",
        "                            predictions[allergen] = min(1.0, predictions[allergen] * 1.3)\n",
        "\n",
        "        # New: Apply negative boosting for allergen-free statements\n",
        "        negative_phrases = [\"free from\", \"no \", \"without\", \"not contain\"]\n",
        "        for allergen in predictions:\n",
        "            allergen_lower = allergen.lower()\n",
        "            for neg_phrase in negative_phrases:\n",
        "                pattern = fr'{neg_phrase}\\s+\\w*\\s*{allergen_lower}'\n",
        "                if re.search(pattern, text):\n",
        "                    predictions[allergen] *= 0.2  # Substantially reduce confidence\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_allergens(self, image):\n",
        "        \"\"\"Enhanced prediction pipeline with multi-stage analysis\"\"\"\n",
        "        # Step 1: Extract and correct text with improved methods\n",
        "        text = self.extract_text(image)\n",
        "        if not text:\n",
        "            return [], text\n",
        "\n",
        "        # Step 2: Get model predictions\n",
        "        inputs = self.tokenizer(\n",
        "            text, padding=True, truncation=True,\n",
        "            max_length=128, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            probs = torch.sigmoid(outputs.logits).numpy()[0]\n",
        "\n",
        "        # Step 3: Create initial predictions\n",
        "        predictions = {\n",
        "            self.allergen_classes[i]: float(prob)\n",
        "            for i, prob in enumerate(probs)\n",
        "        }\n",
        "\n",
        "        # Step 4: Apply enhanced fuzzy matching\n",
        "        fuzzy_matches = self.fuzzy_match(text)\n",
        "        for allergen, conf in fuzzy_matches.items():\n",
        "            predictions[allergen] = max(predictions.get(allergen, 0), conf)\n",
        "\n",
        "        # Step 5: Apply enhanced context rules\n",
        "        predictions = self.apply_context_rules(text, predictions)\n",
        "\n",
        "        # Step 6: Apply thresholds and sort\n",
        "        final = [\n",
        "            (a, f\"{c:.2f}\") for a, c in predictions.items()\n",
        "            if c >= self.thresholds.get(a, self.thresholds['DEFAULT'])\n",
        "        ]\n",
        "        return sorted(final, key=lambda x: float(x[1]), reverse=True), text\n",
        "\n",
        "    def scan_multi_orientation(self, image):\n",
        "        \"\"\"Try multiple orientations for better text capture\"\"\"\n",
        "        best_results = []\n",
        "        best_text = \"\"\n",
        "        best_confidence = 0\n",
        "\n",
        "        # Try original orientation\n",
        "        results, text = self.predict_allergens(image)\n",
        "        total_confidence = sum(float(conf) for _, conf in results)\n",
        "\n",
        "        if total_confidence > best_confidence:\n",
        "            best_results = results\n",
        "            best_text = text\n",
        "            best_confidence = total_confidence\n",
        "\n",
        "        # Try rotations if original results are poor\n",
        "        if total_confidence < 0.5:\n",
        "            for angle in [90, 180, 270]:\n",
        "                rotated = self.rotate_image(image, angle)\n",
        "                rot_results, rot_text = self.predict_allergens(rotated)\n",
        "                rot_confidence = sum(float(conf) for _, conf in rot_results)\n",
        "\n",
        "                if rot_confidence > best_confidence:\n",
        "                    best_results = rot_results\n",
        "                    best_text = rot_text\n",
        "                    best_confidence = rot_confidence\n",
        "\n",
        "        return best_results, best_text\n",
        "\n",
        "    def rotate_image(self, image, angle):\n",
        "        \"\"\"Rotate image by specified angle\"\"\"\n",
        "        height, width = image.shape[:2]\n",
        "        center = (width // 2, height // 2)\n",
        "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "        return cv2.warpAffine(image, rotation_matrix, (width, height))\n",
        "\n",
        "\n",
        "# Multiple upload methods support - choose the one that works in your environment\n",
        "def upload_image(method='file_dialog'):\n",
        "    \"\"\"Provides multiple methods to upload an image\"\"\"\n",
        "    if method == 'file_dialog':\n",
        "        # Standard file dialog approach - works in most environments\n",
        "        root = tk.Tk()\n",
        "        root.withdraw()  # Hide the main window\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            title=\"Select Food Label Image\",\n",
        "            filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tif *.tiff\")]\n",
        "        )\n",
        "        root.destroy()\n",
        "\n",
        "        if not file_path:\n",
        "            print(\"No file selected.\")\n",
        "            return None\n",
        "\n",
        "        return cv2.imread(file_path)\n",
        "\n",
        "    elif method == 'drag_drop':\n",
        "        # Simple drag and drop interface\n",
        "        print(\"Drag and drop your image file here, then press Enter:\")\n",
        "        file_path = input().strip()\n",
        "\n",
        "        # Remove quotes if they exist (common when dragging files in some terminals)\n",
        "        file_path = file_path.strip('\"\\'')\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File not found: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        return cv2.imread(file_path)\n",
        "\n",
        "    elif method == 'clipboard':\n",
        "        # Try to get image from clipboard (requires PIL)\n",
        "        try:\n",
        "            from PIL import ImageGrab\n",
        "            image = ImageGrab.grabclipboard()\n",
        "            if image:\n",
        "                return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "            else:\n",
        "                print(\"No image found in clipboard.\")\n",
        "                return None\n",
        "        except ImportError:\n",
        "            print(\"PIL ImageGrab not available. Try another upload method.\")\n",
        "            return None\n",
        "\n",
        "    elif method == 'camera':\n",
        "        # Capture from webcam\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(0)\n",
        "            if not cap.isOpened():\n",
        "                print(\"Could not open webcam\")\n",
        "                return None\n",
        "\n",
        "            print(\"Press SPACE to capture an image, ESC to cancel\")\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                cv2.imshow('Webcam Capture', frame)\n",
        "\n",
        "                key = cv2.waitKey(1)\n",
        "                if key == 27:  # ESC key\n",
        "                    break\n",
        "                elif key == 32:  # SPACE key\n",
        "                    cap.release()\n",
        "                    cv2.destroyAllWindows()\n",
        "                    return frame\n",
        "\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Camera capture error: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid upload method\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def run_allergen_detection_cli(model, tokenizer, allergen_classes):\n",
        "    \"\"\"Command-line interface for allergen detection\"\"\"\n",
        "    detector = EnhancedAllergenDetector(model, tokenizer, allergen_classes)\n",
        "\n",
        "    print(\"\\n===== Food Label Allergen Scanner =====\\n\")\n",
        "    print(\"Select how you want to upload your image:\")\n",
        "    print(\"1. File browser dialog\")\n",
        "    print(\"2. Drag and drop file path\")\n",
        "    print(\"3. Image from clipboard\")\n",
        "    print(\"4. Capture from webcam\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1-4): \").strip()\n",
        "\n",
        "    methods = {\n",
        "        '1': 'file_dialog',\n",
        "        '2': 'drag_drop',\n",
        "        '3': 'clipboard',\n",
        "        '4': 'camera'\n",
        "    }\n",
        "\n",
        "    method = methods.get(choice, 'file_dialog')\n",
        "    image = upload_image(method)\n",
        "\n",
        "    if image is None:\n",
        "        print(\"Failed to get image. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nScanning image...\\n\")\n",
        "\n",
        "    # Display scan mode options\n",
        "    print(\"Select scan mode:\")\n",
        "    print(\"1. Standard\")\n",
        "    print(\"2. High Accuracy (Slower)\")\n",
        "    print(\"3. Multi-orientation\")\n",
        "\n",
        "    scan_choice = input(\"Enter your choice (1-3): \").strip()\n",
        "\n",
        "    # Process image based on selected mode\n",
        "    if scan_choice == '2':\n",
        "        # High accuracy mode\n",
        "        text = detector.extract_text(image)\n",
        "        inputs = detector.tokenizer(\n",
        "            text, padding=True, truncation=True,\n",
        "            max_length=128, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = detector.model(**inputs)\n",
        "            probs = torch.sigmoid(outputs.logits).numpy()[0]\n",
        "\n",
        "        predictions = {\n",
        "            detector.allergen_classes[i]: float(prob)\n",
        "            for i, prob in enumerate(probs)\n",
        "        }\n",
        "\n",
        "        fuzzy_matches = detector.fuzzy_match(text)\n",
        "        for allergen, conf in fuzzy_matches.items():\n",
        "            predictions[allergen] = max(predictions.get(allergen, 0), conf)\n",
        "\n",
        "        predictions = detector.apply_context_rules(text, predictions)\n",
        "\n",
        "        final = [\n",
        "            (a, f\"{c:.2f}\") for a, c in predictions.items()\n",
        "            if c >= detector.thresholds.get(a, detector.thresholds['DEFAULT'])\n",
        "        ]\n",
        "        predictions = sorted(final, key=lambda x: float(x[1]), reverse=True)\n",
        "\n",
        "    elif scan_choice == '3':\n",
        "        # Multi-orientation mode\n",
        "        predictions, text = detector.scan_multi_orientation(image)\n",
        "    else:\n",
        "        # Standard mode\n",
        "        predictions, text = detector.predict_allergens(image)\n",
        "\n",
        "    # Show the image and processed version\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(detector.preprocess_image(image), cmap='gray')\n",
        "    plt.title(\"Processed for OCR\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \" Extracted Text \".center(50, '-'))\n",
        "    print(text)\n",
        "    print(\"\\n\" + \" Detected Allergens \".center(50, '-'))\n",
        "    if predictions:\n",
        "        for allergen, confidence in predictions:\n",
        "            print(f\"- {allergen} (Confidence: {confidence})\")\n",
        "    else:\n",
        "        print(\"No allergens detected above threshold\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "# Interactive notebook-friendly version\n",
        "def interactive_allergen_check(model, tokenizer, allergen_classes):\n",
        "    \"\"\"Interactive image upload interface for notebooks\"\"\"\n",
        "    detector = EnhancedAllergenDetector(model, tokenizer, allergen_classes)\n",
        "\n",
        "    # Determine environment\n",
        "    try:\n",
        "        import google.colab\n",
        "        in_colab = True\n",
        "    except ImportError:\n",
        "        in_colab = False\n",
        "\n",
        "    if in_colab:\n",
        "        # Colab-specific upload\n",
        "        from google.colab import files\n",
        "\n",
        "        print(\"Upload a food label image:\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            print(\"No file uploaded.\")\n",
        "            return\n",
        "\n",
        "        for filename, content in uploaded.items():\n",
        "            image = cv2.imdecode(np.frombuffer(content, np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "            # Show scan mode selection\n",
        "            from IPython.display import display\n",
        "            import ipywidgets as widgets\n",
        "\n",
        "            scan_mode = widgets.RadioButtons(\n",
        "                options=['Standard', 'High Accuracy (Slower)', 'Multi-orientation'],\n",
        "                value='Standard',\n",
        "                description='Scan Mode:',\n",
        "                disabled=False\n",
        "            )\n",
        "\n",
        "            display(scan_mode)\n",
        "\n",
        "            # Wait for user to select a mode\n",
        "            import time\n",
        "            time.sleep(1)  # Give UI time to render\n",
        "\n",
        "            mode = scan_mode.value\n",
        "\n",
        "            # Process based on selected mode\n",
        "            if mode == 'High Accuracy (Slower)':\n",
        "                # Process using high accuracy mode\n",
        "                text = detector.extract_text(image)\n",
        "                predictions = process_high_accuracy(detector, text)\n",
        "            elif mode == 'Multi-orientation':\n",
        "                # Try multiple orientations\n",
        "                predictions, text = detector.scan_multi_orientation(image)\n",
        "            else:\n",
        "                # Standard processing\n",
        "                predictions, text = detector.predict_allergens(image)\n",
        "\n",
        "            # Display results\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            plt.title(\"Original Image\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(detector.preprocess_image(image), cmap='gray')\n",
        "            plt.title(\"Processed for OCR\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            print(\" Extracted Text \".center(50, '-'))\n",
        "            print(text)\n",
        "            print(\"\\n Detected Allergens \".center(50, '-'))\n",
        "            if predictions:\n",
        "                for allergen, confidence in predictions:\n",
        "                    print(f\"- {allergen} (Confidence: {confidence})\")\n",
        "            else:\n",
        "                print(\"No allergens detected above threshold\")\n",
        "            print(\"=\"*50)\n",
        "            break  # Process only the first uploaded file\n",
        "\n",
        "    else:\n",
        "        # Standard notebook environment - use widgets if available\n",
        "        try:\n",
        "            import ipywidgets as widgets\n",
        "            from IPython.display import display, clear_output\n",
        "\n",
        "            upload_button = widgets.Button(\n",
        "                description='Upload Image',\n",
        "                button_style='primary',\n",
        "                icon='upload'\n",
        "            )\n",
        "\n",
        "            output = widgets.Output()\n",
        "\n",
        "            def on_click(b):\n",
        "                with output:\n",
        "                    clear_output()\n",
        "                    image = upload_image('file_dialog')\n",
        "                    if image is not None:\n",
        "                        # Process and display results\n",
        "                        predictions, text = detector.predict_allergens(image)\n",
        "\n",
        "                        plt.figure(figsize=(12, 6))\n",
        "                        plt.subplot(1, 2, 1)\n",
        "                        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "                        plt.title(\"Original Image\")\n",
        "                        plt.axis('off')\n",
        "\n",
        "                        plt.subplot(1, 2, 2)\n",
        "                        plt.imshow(detector.preprocess_image(image), cmap='gray')\n",
        "                        plt.title(\"Processed for OCR\")\n",
        "                        plt.axis('off')\n",
        "                        plt.show()\n",
        "\n",
        "                        print(\" Extracted Text \".center(50, '-'))\n",
        "                        print(text)\n",
        "                        print(\"\\n Detected Allergens \".center(50, '-'))\n",
        "                        if predictions:\n",
        "                            for allergen, confidence in predictions:\n",
        "                                print(f\"- {allergen} (Confidence: {confidence})\")\n",
        "                        else:\n",
        "                            print(\"No allergens detected above threshold\")\n",
        "                        print(\"=\"*50)\n",
        "\n",
        "            upload_button.on_click(on_click)\n",
        "            display(widgets.VBox([upload_button, output]))\n",
        "\n",
        "        except ImportError:\n",
        "            # Fall back to CLI if widgets aren't available\n",
        "            run_allergen_detection_cli(model, tokenizer, allergen_classes)\n",
        "\n",
        "\n",
        "def process_high_accuracy(detector, text):\n",
        "    \"\"\"Helper function for high accuracy processing\"\"\"\n",
        "    inputs = detector.tokenizer(\n",
        "        text, padding=True, truncation=True,\n",
        "        max_length=128, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = detector.model(**inputs)\n",
        "        probs = torch.sigmoid(outputs.logits).numpy()[0]\n",
        "\n",
        "    predictions = {\n",
        "        detector.allergen_classes[i]: float(prob)\n",
        "        for i, prob in enumerate(probs)\n",
        "    }\n",
        "\n",
        "    fuzzy_matches = detector.fuzzy_match(text)\n",
        "    for allergen, conf in fuzzy_matches.items():\n",
        "        predictions[allergen] = max(predictions.get(allergen, 0), conf)\n",
        "\n",
        "    predictions = detector.apply_context_rules(text, predictions)\n",
        "\n",
        "    final = [\n",
        "        (a, f\"{c:.2f}\") for a, c in predictions.items()\n",
        "        if c >= detector.thresholds.get(a, detector.thresholds['DEFAULT'])\n",
        "    ]\n",
        "    return sorted(final, key=lambda x: float(x[1]), reverse=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in interactive environment\n",
        "    is_interactive = hasattr(sys, 'ps1')\n",
        "\n",
        "    # Define allergen classes\n",
        "    allergen_classes = [\n",
        "        'Milk', 'Eggs', 'Peanuts', 'Tree Nuts', 'Soy', 'Wheat',\n",
        "        'Fish', 'Shellfish', 'Sesame', 'Gluten', 'Sulphur Dioxide'\n",
        "    ]\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    try:\n",
        "        from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "        try:\n",
        "            MODEL_PATH = \"/content/drive/MyDrive/allergen_detection\"\n",
        "            model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "            tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "        except:\n",
        "            # If local model not available, load a pretrained model\n",
        "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "            print(\"Warning: Using placeholder model. Results may not be accurate.\")\n",
        "\n",
        "        # Run the appropriate interface\n",
        "        if is_interactive:\n",
        "            interactive_allergen_check(model, tokenizer, allergen_classes)\n",
        "        else:\n",
        "            run_allergen_detection_cli(model, tokenizer, allergen_classes)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        print(\"Running demo mode with image upload only...\")\n",
        "\n",
        "        # Fallback to just displaying the image without allergen detection\n",
        "        image = upload_image()\n",
        "        if image is not None:\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            plt.title(\"Uploaded Image (Model could not be loaded for scanning)\")\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "QFC7s7BHuX3M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}